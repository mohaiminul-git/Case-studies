{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff44539-08aa-414b-9693-c35586dabf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels==0.14.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d34d6-5c04-43da-bdf2-0ee0ace31c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "print(f\"Memory used: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ade3c-c190-4e32-8a07-7372c33c5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from statsmodels.multivariate.multivariate_ols import _multivariate_ols_fit\n",
    "from patsy import dmatrix\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3271b-ae46-45af-959d-9fcb277b7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dct=os.getcwd()\n",
    "csv_files=glob.glob(f\"{current_dct}/*.csv\")\n",
    "dfs=[]\n",
    "for files in csv_files:\n",
    "    df=pd.read_csv(files)\n",
    "    df[\"house_hold\"] = os.path.splitext(os.path.basename(files))[0].split(\"H\")[1]\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329fca3-7a78-4203-ae07-336b94bf5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['index'] = pd.to_datetime(data['index']) \n",
    "data[\"house_hold\"] = data[\"house_hold\"].astype(\"int\")\n",
    "data = data.drop(columns=[\"PUMPE_TOT\",\"TEMPERATURE:TOTAL\"])\n",
    "data[\"day\"]=data[\"index\"].dt.dayofyear\n",
    "data['weekday'] = data['index'].dt.day_name().astype(\"category\")\n",
    "data[\"is_weekday\"]=data['index'].apply(lambda x: 0 if x.weekday() < 5 else 1)\n",
    "data[\"time_of_day\"] = data[\"index\"].dt.time\n",
    "data[\"season\"] = data[\"day\"].apply(lambda x: math.sin((x - (31+28+24))/365 * 2*math.pi))\n",
    "#data['is_summer'] = (data['season'] >= 0).astype(int)\n",
    "data[\"is_weekday\"]=data[\"is_weekday\"].astype(\"category\")\n",
    "#data['is_summer'] = data['is_summer'].astype(\"category\")\n",
    "\n",
    "data[\"hour\"] = data[\"index\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029e87d-92b9-4a0f-ac2e-44d618cc2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90e7b1-0a27-4558-a920-bc66ccdede4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly = (\n",
    "    data.groupby(['house_hold', 'day', 'hour'])\n",
    "      .agg({\n",
    "          'HAUSHALT_TOT': 'mean',\n",
    "          'weekday': 'first',    # keep weekday info per day\n",
    "          'season': 'first',\n",
    "          'is_weekday': 'first' ,# keep season info per day\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(data_hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9392aa-4890-4000-b91c-ea1b4c772312",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ba5cc-5176-48b4-bd69-7db5cbcb754e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for day_ in range(1, 366):\n",
    "    tmp_0 = data[(data[\"day\"] == day_) & (data[\"house_hold\"] == 4)].reset_index()\n",
    "    \n",
    "    tmp = data_hourly[(data_hourly[\"day\"] == day_) & (data_hourly[\"house_hold\"] == 4)].reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(tmp_0.index, tmp_0[\"HAUSHALT_TOT\"], label='Raw data (15-min)')\n",
    "    \n",
    "    plt.plot(tmp[\"hour\"], tmp[\"HAUSHALT_TOT\"], label='Hourly aggregation', marker='o')\n",
    "    \n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylabel(\"HAUSHALT_TOT\")\n",
    "    plt.title(f\"Household 4, Day {day_}\")\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a60c29-56d7-4fb4-8dfa-cc4cf7d3f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_chunks = []\n",
    "\n",
    "for household_id, group in data_hourly.groupby(\"house_hold\"):\n",
    "    temp = group.copy()\n",
    "\n",
    "    # Rolling computations\n",
    "    temp[\"v1\"] = temp.groupby(\"day\")[\"HAUSHALT_TOT\"]\\\n",
    "                     .transform(lambda x: x.rolling(window=5, center=True, min_periods=1).mean())\n",
    "    temp[\"v2\"] = temp.groupby([\"weekday\", \"hour\"])[\"HAUSHALT_TOT\"]\\\n",
    "                     .transform(lambda x: x.rolling(window=5, center=True, min_periods=1).mean())\n",
    "\n",
    "    # Pivot per household\n",
    "    pivot = temp.pivot_table(\n",
    "        index=[\"house_hold\", \"day\", \"weekday\", \"season\", \"is_weekday\"],\n",
    "        columns='hour',\n",
    "        values=['v1', 'v2']\n",
    "    )\n",
    "\n",
    "    pivot.columns = [f'{val}_hour_{hour}' for val, hour in pivot.columns]\n",
    "    result_chunks.append(pivot.reset_index())\n",
    "\n",
    "# Combine all chunks\n",
    "df = pd.concat(result_chunks, ignore_index=True)\n",
    "df[\"house_hold\"] = df[\"house_hold\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17925af-3262-4d9f-9c1d-775b1fd8d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter only v1 columns\n",
    "v1_cols = [col for col in df.columns if col.startswith(\"v1_hour_\")]\n",
    "\n",
    "# Calculate missing percentage per household for v1 only\n",
    "missing_summary_v1 = df.groupby(\"house_hold\")[v1_cols].apply(\n",
    "    lambda group: group.isnull().sum().sum() / group.size * 100\n",
    ").reset_index(name=\"missing_percentage\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(missing_summary_v1['house_hold'].astype(str), missing_summary_v1['missing_percentage'], color='salmon')\n",
    "plt.xlabel('Household')\n",
    "plt.ylabel('Missing Value Percentage (%)')\n",
    "plt.title('Missing Value Percentage by Household')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d2c5a-bf49-4c7a-9b7f-48f73e7df8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_hourly[\"v1\"] = data_hourly.groupby([\"house_hold\", \"day\"])[\"HAUSHALT_TOT\"]\\\n",
    "    .transform(lambda x: x.rolling(window=5, center=True, min_periods=1).mean())\n",
    "\n",
    "data_hourly[\"v2\"]=data_hourly.groupby([\"house_hold\", \"weekday\", \"hour\"])[\"HAUSHALT_TOT\"]\\\n",
    "    .transform(lambda x: x.rolling(window=5, center=True, min_periods=1).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ded136-301d-4518-b74d-6936090760ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for day_ in range(1, 366): #if we want to plot for each day of the year\n",
    "    tmp = data_hourly[(data_hourly[\"day\"] == day_) & (data_hourly[\"house_hold\"] == 4)].reset_index()\n",
    "    #plt.plot(tmp.index, tmp[\"HAUSHALT_TOT\"], label = 'Haushalt total')\n",
    "    plt.plot(tmp[\"hour\"], tmp[\"HAUSHALT_TOT\"], label = 'raw_data')\n",
    "    plt.plot(tmp[\"hour\"], tmp[\"v1\"], label = 'Withinday smoothing')\n",
    "    plt.plot(tmp[\"hour\"], tmp[\"v2\"], label = 'Weekday smoothing')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylabel(\"Haushalt total\")\n",
    "    plt.title(\"Day: \"+ str(day_))\n",
    "    #plt.xticks(ticks=tmp.index, labels=tmp[\"hour\"])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5413ba-4895-43f1-b0aa-c7cbb59514f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_cols_v1 = [f'v1_hour_{h}' for h in range (0,24)]\n",
    "df[hour_cols_v1]=df[hour_cols_v1].interpolate(axis=1, method='linear', limit_direction='both')\n",
    "hour_cols_v2 = [f'v2_hour_{h}' for h in range (0,24)]\n",
    "df[hour_cols_v2]=df[hour_cols_v2].interpolate(axis=1, method='linear', limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec87156-0b83-4448-a426-2f93b5ac50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "household_3=np.random.choice(df[\"house_hold\"].unique(),3,replace=False)\n",
    "df_3=df[df[\"house_hold\"].isin(household_3)].copy()\n",
    "df_30=df[~df[\"house_hold\"].isin(household_3)].copy()\n",
    "for col in [\"house_hold\", \"weekday\", \"is_weekday\"]:\n",
    "    df_30[col] = df_30[col].cat.remove_unused_categories()\n",
    "    df_3[col] = df_3[col].cat.remove_unused_categories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7128e-6a7a-42a1-9462-42e6aae9d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3[\"house_hold\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43e77d-4c43-4dd0-a447-872fd9fec5ed",
   "metadata": {},
   "source": [
    "## manova for v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68e9b0-7243-4c0e-9225-6a7286a0c5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependent_vars_v1 = '+'.join([f'v1_hour_{h}' for h in range (0,24)])\n",
    "formula1=f'{dependent_vars_v1} ~ -1+ house_hold*weekday*season'\n",
    "formula2=f'{dependent_vars_v1} ~ -1+house_hold*is_weekday*season'\n",
    "manova_v1 = MANOVA.from_formula(formula1, data=df_30)\n",
    "manova_v12 = MANOVA.from_formula(formula2, data=df_30)\n",
    "print(manova_v1.mv_test())\n",
    "print(manova_v12.mv_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86afd3-0754-4348-aeac-c8ee6d0c79db",
   "metadata": {},
   "source": [
    "## manova for v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0acae22-01d0-4ebc-b519-f236f43f5bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependent_vars_v2 = '+'.join([f'v2_hour_{h}' for h in range (0,24)])\n",
    "formula1=f'{dependent_vars_v2} ~ -1+ house_hold*weekday*season'\n",
    "formula2=f'{dependent_vars_v2} ~ -1+house_hold*is_weekday*season'\n",
    "manova_v2 = MANOVA.from_formula(formula1, data=df_30)\n",
    "manova_v22 = MANOVA.from_formula(formula2, data=df_30)\n",
    "print(manova_v2.mv_test())\n",
    "print(manova_v22.mv_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47e261-4410-46ca-b09d-49fbfeaf3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wilks_lambda(manova_result):\n",
    "    wilks_dict = {}\n",
    "\n",
    "    for effect_name, effect_result in manova_result.mv_test().results.items():\n",
    "        stat_df = effect_result['stat']\n",
    "        wilks_row = stat_df.loc[\"Wilks' lambda\"]\n",
    "\n",
    "        wilks_dict[effect_name] = {\n",
    "            'Wilks Lambda': wilks_row['Value'],\n",
    "            'Num DF': wilks_row['Num DF'],\n",
    "            'F Value': wilks_row['F Value'],\n",
    "            'Pr > F': f\"{wilks_row['Pr > F']:.4f}\"\n",
    "        }\n",
    "\n",
    "    return wilks_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbc4f6-3726-44c6-a353-2dbef3afcc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilks1 = extract_wilks_lambda(manova_v1)\n",
    "wilks2 = extract_wilks_lambda(manova_v12)\n",
    "\n",
    "import pandas as pd\n",
    "df_wilks1 = pd.DataFrame.from_dict(wilks1, orient='index')\n",
    "df_wilks2 = pd.DataFrame.from_dict(wilks2, orient='index')\n",
    "\n",
    "print(\"MANOVA 1 Wilks' Lambda:\")\n",
    "print(df_wilks1)\n",
    "\n",
    "print(\"\\nMANOVA 2 Wilks' Lambda:\")\n",
    "print(df_wilks2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1614d-c2ab-4d34-aefe-c63dcc53a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilks1 = extract_wilks_lambda(manova_v2)\n",
    "wilks2 = extract_wilks_lambda(manova_v22)\n",
    "\n",
    "import pandas as pd\n",
    "df_wilks1 = pd.DataFrame.from_dict(wilks1, orient='index')\n",
    "df_wilks2 = pd.DataFrame.from_dict(wilks2, orient='index')\n",
    "\n",
    "print(\"MANOVA 1 Wilks' Lambda:\")\n",
    "print(df_wilks1)\n",
    "\n",
    "print(\"\\nMANOVA 2 Wilks' Lambda:\")\n",
    "print(df_wilks2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e455bca-e6ac-428c-b6a1-310c84e974a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic_mv(model,Y, X):\n",
    "\n",
    "    n, R = Y.shape\n",
    "    q = X.shape[1]\n",
    "\n",
    "    # Residuals\n",
    "    #residuals = Y - np.dot(X,model[0]) # (n x R)\n",
    "    residuals = Y - (X @ model[0])\n",
    "\n",
    "    # Residual covariance matrix estimate (ML estimate)\n",
    "    Sigma = model[3] / model[1]\n",
    "\n",
    "    # Check if Sigma_ml is positive definite\n",
    "    sign, logdet = np.linalg.slogdet(Sigma)\n",
    "    if sign <= 0:\n",
    "        raise ValueError(\"Covariance matrix is not positive definite.\")\n",
    "\n",
    "    # Mahalanobis distance squared for each observation\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    maha2 = np.sum(residuals @ Sigma_inv * residuals, axis=1)\n",
    "    #maha2 = np.einsum('ij,jk,ik->i', residuals, Sigma_inv, residuals)\n",
    "\n",
    "\n",
    "    # Log-likelihood\n",
    "    ll = - (n * R / 2) * np.log(2 * np.pi) - (n / 2) * logdet - 0.5 * np.sum(maha2)\n",
    "\n",
    "    # Number of parameters: q*R regression + R(R+1)/2 covariance\n",
    "    k = q * R + (R * (R + 1)) / 2\n",
    "\n",
    "    # AIC\n",
    "    AIC = 2 * k - 2 * ll\n",
    "\n",
    "    return AIC\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff3c28-aaf8-4ead-b67f-725151a894ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_main_effect_7 =pd.get_dummies(df_30[[\"house_hold\", \"weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "X_interaction_7 = dmatrix(\"house_hold * weekday * season\", data=df_30, return_type=\"dataframe\")\n",
    "X_main_effect_2 = pd.get_dummies(df_30[[\"house_hold\", \"is_weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "X_interaction_2 =dmatrix(\"house_hold * is_weekday * season\", data=df_30, return_type=\"dataframe\")\n",
    "Y = df_30[hour_cols_v1].to_numpy()\n",
    "model_1=_multivariate_ols_fit(Y,X_main_effect_7)\n",
    "model_2=_multivariate_ols_fit(Y,X_interaction_7)\n",
    "model_3=_multivariate_ols_fit(Y,X_main_effect_2)\n",
    "model_4=_multivariate_ols_fit(Y,X_interaction_2)\n",
    "#params_1, df_resid_1, inv_cov_1, sscpr_1 = model_1\n",
    "print(f\"X_main_effect_7 AIC:{compute_aic_mv(model_1,Y, X_main_effect_7)}\")\n",
    "print(f\"X_interaction_7 AIC:{compute_aic_mv(model_2,Y, X_interaction_7)}\")\n",
    "print(f\"X_main_effect_2 AIC:{compute_aic_mv(model_3,Y, X_main_effect_2)}\")\n",
    "print(f\"X_interaction_2 AIC:{compute_aic_mv(model_4,Y, X_interaction_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b323c-23a3-4177-aa47-3e006a98deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_main_effect_7 =pd.get_dummies(df_30[[\"house_hold\", \"weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "X_interaction_7 = dmatrix(\"house_hold * weekday * season\", data=df_30, return_type=\"dataframe\")\n",
    "X_main_effect_2 = pd.get_dummies(df_30[[\"house_hold\", \"is_weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "X_interaction_2 =dmatrix(\"house_hold * is_weekday * season\", data=df_30, return_type=\"dataframe\")\n",
    "Y = df_30[hour_cols_v2].to_numpy()\n",
    "model_1=_multivariate_ols_fit(Y,X_main_effect_7)\n",
    "model_2=_multivariate_ols_fit(Y,X_interaction_7)\n",
    "model_3=_multivariate_ols_fit(Y,X_main_effect_2)\n",
    "model_4=_multivariate_ols_fit(Y,X_interaction_2)\n",
    "#params_1, df_resid_1, inv_cov_1, sscpr_1 = model_1\n",
    "print(f\"X_main_effect_7 AIC:{compute_aic_mv(model_1,Y, X_main_effect_7)}\")\n",
    "print(f\"X_interaction_7 AIC:{compute_aic_mv(model_2,Y, X_interaction_7)}\")\n",
    "print(f\"X_main_effect_2 AIC:{compute_aic_mv(model_3,Y, X_main_effect_2)}\")\n",
    "print(f\"X_interaction_2 AIC:{compute_aic_mv(model_4,Y, X_interaction_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f625d-85c3-4d08-a1b0-92aba5ecab1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house=df_30[df_30[\"house_hold\"]==house_id]\n",
    "    manova_3 = MANOVA.from_formula(f'{dependent_vars_v1} ~ -1+ is_weekday*season', data=house)\n",
    "    print(f\"houseld_hold{house_id} :{manova_3.mv_test()}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e57a26-4d59-4d1a-858e-522fe56e3dae",
   "metadata": {},
   "source": [
    "## seperate houshold _v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14f8d6-258d-4536-b659-9e7b559b1a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house=df_30[df_30[\"house_hold\"]==house_id]\n",
    "    X_main_effect_7_separate =pd.get_dummies(house[[\"weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "    X_interaction_7_separate = dmatrix(\" weekday * season\", data=house, return_type=\"dataframe\")\n",
    "    X_main_effect_2_separate= pd.get_dummies(house[[\"is_weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "    X_interaction_2_separate =dmatrix(\" is_weekday * season\", data=house, return_type=\"dataframe\")\n",
    "    hour_cols_v1 = [f'v1_hour_{h}' for h in range (0,24)]\n",
    "    Y = house[hour_cols_v1].to_numpy()\n",
    "    model_1_separat=_multivariate_ols_fit(Y,X_main_effect_7_separate)\n",
    "    model_2_separat=_multivariate_ols_fit(Y,X_interaction_7_separate)\n",
    "    model_3_separat=_multivariate_ols_fit(Y,X_main_effect_2_separate)\n",
    "    model_4_separat=_multivariate_ols_fit(Y,X_interaction_2_separate)\n",
    "    #params_1, df_resid_1, inv_cov_1, sscpr_1 = model_1\n",
    "    print(f\"{house_id}X_main_effect_7 AIC:{compute_aic_mv(model_1_separat,Y, X_main_effect_7_separate)}\")\n",
    "    print(f\"{house_id}X_interaction_7 AIC:{compute_aic_mv(model_2_separat,Y, X_interaction_7_separate)}\")\n",
    "    print(f\"{house_id}X_main_effect_2 AIC:{compute_aic_mv(model_3_separat,Y, X_main_effect_2_separate)}\")\n",
    "    print(f\"{house_id}X_interaction_2 AIC:{compute_aic_mv(model_4_separat,Y, X_interaction_2_separate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814523c1-b9cb-4c0d-ac4f-8b825f32a385",
   "metadata": {},
   "source": [
    "## separete household v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881a9c1-0f52-4c58-9575-d3570b1e847d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house=df_30[df_30[\"house_hold\"]==house_id]\n",
    "    manova_3 = MANOVA.from_formula(f'{dependent_vars_v2} ~ -1+ is_weekday*season', data=house)\n",
    "    print(f\"{house_id} :{manova_3.mv_test()}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213dd6de-b48f-451a-be5f-c2195dd853f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house=df_30[df_30[\"house_hold\"]==house_id]\n",
    "    X_main_effect_7_separate =pd.get_dummies(house[[\"weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "    X_interaction_7_separate = dmatrix(\" weekday * season\", data=house, return_type=\"dataframe\")\n",
    "    X_main_effect_2_separate= pd.get_dummies(house[[\"is_weekday\", \"season\"]], drop_first=True).astype(\"float\")\n",
    "    X_interaction_2_separate =dmatrix(\" is_weekday * season\", data=house, return_type=\"dataframe\")\n",
    "    hour_cols_v2 = [f'v2_hour_{h}' for h in range (0,24)]\n",
    "    Y = house[hour_cols_v2].to_numpy()\n",
    "    model_1_separat=_multivariate_ols_fit(Y,X_main_effect_7_separate)\n",
    "    model_2_separat=_multivariate_ols_fit(Y,X_interaction_7_separate)\n",
    "    model_3_separat=_multivariate_ols_fit(Y,X_main_effect_2_separate)\n",
    "    model_4_separat=_multivariate_ols_fit(Y,X_interaction_2_separate)\n",
    "    #params_1, df_resid_1, inv_cov_1, sscpr_1 = model_1\n",
    "    print(f\"{house_id}X_main_effect_7 AIC:{compute_aic_mv(model_1_separat,Y, X_main_effect_7_separate)}\")\n",
    "    print(f\"{house_id}X_interaction_7 AIC:{compute_aic_mv(model_2_separat,Y, X_interaction_7_separate)}\")\n",
    "    print(f\"{house_id}X_main_effect_2 AIC:{compute_aic_mv(model_3_separat,Y, X_main_effect_2_separate)}\")\n",
    "    print(f\"{house_id}X_interaction_2 AIC:{compute_aic_mv(model_4_separat,Y, X_interaction_2_separate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a541c53-abf2-49ae-9760-503480c32ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from patsy import dmatrix\n",
    "\n",
    "# will hold one row per household\n",
    "rows = []\n",
    "\n",
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house = df_30[df_30[\"house_hold\"] == house_id]\n",
    "    \n",
    "    # design matrices\n",
    "    X_main7     = pd.get_dummies(house[[\"weekday\", \"season\"]], drop_first=True).astype(float)\n",
    "    X_int7      = dmatrix(\"weekday * season\",      data=house, return_type=\"dataframe\")\n",
    "    X_main2     = pd.get_dummies(house[[\"is_weekday\", \"season\"]], drop_first=True).astype(float)\n",
    "    X_int2      = dmatrix(\"is_weekday * season\",   data=house, return_type=\"dataframe\")\n",
    "    \n",
    "    # response\n",
    "    hour_cols   = [f'v1_hour_{h}' for h in range(24)]\n",
    "    Y           = house[hour_cols].to_numpy()\n",
    "    \n",
    "    # fit each\n",
    "    m1 = _multivariate_ols_fit(Y, X_main7)\n",
    "    m2 = _multivariate_ols_fit(Y, X_int7)\n",
    "    m3 = _multivariate_ols_fit(Y, X_main2)\n",
    "    m4 = _multivariate_ols_fit(Y, X_int2)\n",
    "    \n",
    "    # compute AICs\n",
    "    aic1 = compute_aic_mv(m1, Y, X_main7)\n",
    "    aic2 = compute_aic_mv(m2, Y, X_int7)\n",
    "    aic3 = compute_aic_mv(m3, Y, X_main2)\n",
    "    aic4 = compute_aic_mv(m4, Y, X_int2)\n",
    "    \n",
    "    # flag if model 4 (2-level interaction) is best\n",
    "    best_aic = min(aic1, aic2, aic3, aic4)\n",
    "    interaction_needed_2 = (aic4 == best_aic)\n",
    "    \n",
    "    rows.append({\n",
    "        \"house_hold\":               house_id,\n",
    "        \"AIC_main7\":                aic1,\n",
    "        \"AIC_interaction7\":         aic2,\n",
    "        \"AIC_main2\":                aic3,\n",
    "        \"AIC_interaction2\":         aic4,\n",
    "        \"use_interaction2\":         interaction_needed_2\n",
    "    })\n",
    "\n",
    "# assemble into a DataFrame\n",
    "df_model_selection = pd.DataFrame(rows)\n",
    "\n",
    "print(df_model_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e833a-819d-440a-9542-884d6eef5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from patsy import dmatrix\n",
    "\n",
    "# will hold one row per household\n",
    "rows = []\n",
    "\n",
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house = df_30[df_30[\"house_hold\"] == house_id]\n",
    "    \n",
    "    # design matrices\n",
    "    X_main7     = pd.get_dummies(house[[\"weekday\", \"season\"]], drop_first=True).astype(float)\n",
    "    X_int7      = dmatrix(\"weekday * season\",      data=house, return_type=\"dataframe\")\n",
    "    X_main2     = pd.get_dummies(house[[\"is_weekday\", \"season\"]], drop_first=True).astype(float)\n",
    "    X_int2      = dmatrix(\"is_weekday * season\",   data=house, return_type=\"dataframe\")\n",
    "    \n",
    "    # response\n",
    "    hour_cols   = [f'v2_hour_{h}' for h in range(24)]\n",
    "    Y           = house[hour_cols].to_numpy()\n",
    "    \n",
    "    # fit each\n",
    "    m1 = _multivariate_ols_fit(Y, X_main7)\n",
    "    m2 = _multivariate_ols_fit(Y, X_int7)\n",
    "    m3 = _multivariate_ols_fit(Y, X_main2)\n",
    "    m4 = _multivariate_ols_fit(Y, X_int2)\n",
    "    \n",
    "    # compute AICs\n",
    "    aic1 = compute_aic_mv(m1, Y, X_main7)\n",
    "    aic2 = compute_aic_mv(m2, Y, X_int7)\n",
    "    aic3 = compute_aic_mv(m3, Y, X_main2)\n",
    "    aic4 = compute_aic_mv(m4, Y, X_int2)\n",
    "    \n",
    "    # flag if model 4 (2-level interaction) is best\n",
    "    best_aic = min(aic1, aic2, aic3, aic4)\n",
    "    interaction_needed_4 = (aic2 == best_aic)\n",
    "    \n",
    "    rows.append({\n",
    "        \"house_hold\":               house_id,\n",
    "        \"AIC_main7\":                aic1,\n",
    "        \"AIC_interaction7\":         aic2,\n",
    "        \"AIC_main2\":                aic3,\n",
    "        \"AIC_interaction2\":         aic4,\n",
    "        \"use_interaction_7\":         interaction_needed_4\n",
    "    })\n",
    "\n",
    "# assemble into a DataFrame\n",
    "df_model_selection = pd.DataFrame(rows)\n",
    "\n",
    "print(df_model_selection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0f7cc-9f6d-4e53-8471-9666f3fed11d",
   "metadata": {},
   "source": [
    "##silhutte_plot_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3d003-a4bf-4e2c-99ce-e5f39ae27bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def plot_silhouette_scores_hierarchical(param_dict, title=\"Silhouette Score vs Number of Clusters (Hierarchical)\"):\n",
    "    param_vectors = []\n",
    "\n",
    "    # Flatten coefficient matrices\n",
    "    for coef_matrix in param_dict.values():\n",
    "        param_vectors.append(coef_matrix)\n",
    "\n",
    "    # Stack and scale\n",
    "    X = np.vstack(param_vectors)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in range(2, 7):\n",
    "        model = AgglomerativeClustering(n_clusters=k)\n",
    "        labels = model.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"k = {k}, Silhouette Score = {score:.4f}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(2, 7), silhouette_scores, marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Number of Clusters (k)\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0d8ef-28a6-4c8c-ae8d-4b1e2ca286a3",
   "metadata": {},
   "source": [
    "#clustering_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc6abe-660a-4381-952f-4a8a3da0836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_pca(param_dict, n_clusters=2, n_neighbors=3):\n",
    "    house_ids = []\n",
    "    param_vectors = []\n",
    "\n",
    "    # Flatten coefficient matrices\n",
    "    for house_id, coef_matrix in param_dict.items():\n",
    "        flat = coef_matrix.flatten()\n",
    "        param_vectors.append(flat)\n",
    "        house_ids.append(house_id)\n",
    "\n",
    "    # Feature matrix\n",
    "    X = np.vstack(param_vectors)\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Hierarchical clustering\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "    # Create DataFrame of results\n",
    "    cluster_df = pd.DataFrame({\n",
    "        \"house_id\": house_ids,\n",
    "        \"cluster\": labels\n",
    "    })\n",
    "\n",
    "    # Train classifier (KNN)\n",
    "    classifier = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    classifier.fit(X_scaled, labels)\n",
    "\n",
    "    # PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    return {\n",
    "        \"cluster_df\": cluster_df,\n",
    "        \"pca_coordinates\": X_pca,\n",
    "        \"labels\": labels,\n",
    "        \"scaler\": scaler,\n",
    "        \"classifier\": classifier,  # trained KNN\n",
    "        \"model\": hierarchical\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cab48-9668-44b9-a9ba-35686e237d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_30 = {}\n",
    "\n",
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house = df_30[df_30[\"house_hold\"] == house_id].copy()\n",
    "    house[\"is_weekday\"] = house[\"is_weekday\"].astype(\"category\")\n",
    "\n",
    "\n",
    "    # Design matrix without intercept (remove intercept with -1)\n",
    "    X_interaction_2_separate = dmatrix(\n",
    "        \"C(is_weekday) * season \", data=house, return_type=\"dataframe\"\n",
    "    )\n",
    "\n",
    "\n",
    "    hour_cols_v1 = [f'v1_hour_{h}' for h in range (0,24)]\n",
    "    Y = house[hour_cols_v1].to_numpy()\n",
    "\n",
    "    try:\n",
    "        model_3 = _multivariate_ols_fit(Y, X_interaction_2_separate)\n",
    "        param_30[house_id] = model_3[0]\n",
    "\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipped household {house_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639ea77-6b87-42e5-93c0-660ace2fef2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac488f-4ca4-48b7-a41e-992a8f619a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_scores_hierarchical(param_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84e34b-d0f6-4798-b73b-caf5801dfda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_with_week_weekend=cluster_and_pca(param_30)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "X_pca = clusters_with_week_weekend[\"pca_coordinates\"]\n",
    "labels = clusters_with_week_weekend[\"labels\"]\n",
    "\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Set1', s=50)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Households clustered with weekdays and weekends')\n",
    "ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25a7f6-231a-4eeb-b521-be7222d756ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_with_week_weekend = cluster_and_pca(param_30)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "X_pca = clusters_with_week_weekend[\"pca_coordinates\"]\n",
    "labels = clusters_with_week_weekend[\"labels\"]\n",
    "\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Set1', s=50)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Households clustered with full model')\n",
    "ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "# Add silhouette score annotation\n",
    "ax.text(0.95, 0.05, \"Silhouette Score = 0.69\", transform=ax.transAxes,\n",
    "        fontsize=10, ha='right', va='bottom', bbox=dict(facecolor='white', edgecolor='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3048fc-e2c2-46ec-a82b-79f0aa35c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=clusters_with_week_weekend[\"cluster_df\"]\n",
    "Y_0=X[X[\"cluster\"] == 0]\n",
    "Y_1=X[X[\"cluster\"] == 1]\n",
    "print(Y_0.shape)\n",
    "print(Y_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e4c7d-3556-4764-af8f-eec3f589a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param_week_0 = {}\n",
    "param_week_1 = {}\n",
    "weekday_predict_df={}\n",
    "weekend_predict_df={}\n",
    "\n",
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house = df_30[df_30[\"house_hold\"] == house_id]\n",
    "\n",
    "    house_0 = house[house[\"is_weekday\"] == 0]\n",
    "    house_1 = house[house[\"is_weekday\"] == 1]\n",
    "\n",
    "    # Add intercept (constant term) to the season variable\n",
    "    X_main_effect_2_separate_0 = sm.add_constant(house_0[[\"season\"]].reset_index(drop=True))\n",
    "    X_main_effect_2_separate_1 = sm.add_constant(house_1[[\"season\"]].reset_index(drop=True))\n",
    "\n",
    "    hour_cols_v1 = [f'v1_hour_{h}' for h in range(24)]\n",
    "    Y_0 = house_0[hour_cols_v1].to_numpy()\n",
    "    Y_1 = house_1[hour_cols_v1].to_numpy()\n",
    "    \n",
    "    model_3_0 = _multivariate_ols_fit(Y_0, X_main_effect_2_separate_0)\n",
    "    model_3_1 = _multivariate_ols_fit(Y_1, X_main_effect_2_separate_1)\n",
    "    \n",
    "    param_week_0[house_id] = model_3_0[0]\n",
    "    param_week_1[house_id] = model_3_1[0]\n",
    "    weekday_predict_df[house_id] = X_main_effect_2_separate_0 @ model_3_0[0]\n",
    "    weekend_predict_df[house_id] = X_main_effect_2_separate_1 @ model_3_1[0]\n",
    "\n",
    "\n",
    "    pred_0 = pd.DataFrame(X_main_effect_2_separate_0 @ model_3_0[0])\n",
    "    pred_0.columns = hour_cols_v1\n",
    "\n",
    "    pred_0[\"house_hold\"] = house_id\n",
    "    pred_0[\"day\"] = house_0[\"day\"].values\n",
    "    weekday_predict_df[house_id] = pred_0\n",
    "\n",
    "    pred_1 = pd.DataFrame(X_main_effect_2_separate_1 @ model_3_1[0])\n",
    "    pred_1.columns = hour_cols_v1\n",
    "\n",
    "    pred_1[\"house_hold\"] = house_id\n",
    "    pred_1[\"day\"] = house_1[\"day\"].values\n",
    "    weekend_predict_df[house_id] = pred_1\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134c7ef-abf4-401f-aa20-54d066754d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekend_predict_df = pd.concat(weekend_predict_df.values(), ignore_index=True)\n",
    "weekday_predict_df = pd.concat(weekday_predict_df.values(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645db05-f13b-4266-8504-cb736f290a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekend_predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d2d37-eed4-471a-a809-1b71bea6dfac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "\n",
    "# # Example usage:\n",
    "# plot_silhouette_scores(param_week_0, title=\"Silhouette Scores for Weekdays\")\n",
    "# # or\n",
    "# plot_silhouette_scores(param_week_1, title=\"Silhouette Scores for Weekends\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e037789-b0ac-434a-a138-f27590b1721a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_silhouette_scores_hierarchical(param_week_0, title=\"Silhouette Scores for Weekdays\")\n",
    "plot_silhouette_scores_hierarchical(param_week_1, title=\"Silhouette Scores for Weekends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ccf00-3ef2-4dd8-92cf-7561ef3bf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weekday= cluster_and_pca(param_week_0)\n",
    "weekend=cluster_and_pca(param_week_1)\n",
    "#weekend[\"cluster_df\"][\"cluster\"] = weekend[\"cluster_df\"][\"cluster\"].map({0: 1, 1: 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f627d-d2d9-4243-a74e-c36d5b602305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekend[\"cluster_df\"][weekend[\"cluster_df\"][\"cluster\"] == 0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5426a-4b08-4147-985a-28e4a0c56985",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday[\"cluster_df\"][weekday[\"cluster_df\"][\"cluster\"] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13446aa0-60a1-4840-9816-9257d697e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import cdist\n",
    "# import numpy as np\n",
    "\n",
    "# # Step 1: Extract cluster centers\n",
    "# weekday_centers = weekday[\"kmeans_model\"].cluster_centers_\n",
    "# weekend_centers = weekend[\"kmeans_model\"].cluster_centers_\n",
    "\n",
    "# # Step 2: Calculate pairwise distances\n",
    "# distance_matrix = cdist(weekday_centers, weekend_centers)\n",
    "\n",
    "# # Step 3: Determine optimal alignment (weekday → weekend cluster index)\n",
    "# weekday_to_weekend = distance_matrix.argmin(axis=1)\n",
    "\n",
    "# # Step 4: Invert the mapping to remap weekend labels to weekday cluster meaning\n",
    "# weekend_remap = {weekend_idx: weekday_idx for weekday_idx, weekend_idx in enumerate(weekday_to_weekend)}\n",
    "\n",
    "# # Step 5: Apply remapping to weekend labels\n",
    "# weekend[\"labels\"] = np.vectorize(weekend_remap.get)(weekend[\"labels\"])\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# for ax, data, label in zip(axs, [weekday, weekend], [\"Weekday\", \"Weekend\"]):\n",
    "#     X_pca = data[\"pca_coordinates\"]\n",
    "#     clusters = data[\"labels\"]\n",
    "    \n",
    "#     scatter = ax.scatter(\n",
    "#         X_pca[:, 0], \n",
    "#         X_pca[:, 1], \n",
    "#         c=clusters, \n",
    "#         cmap='Set1',  # Color scheme changed here\n",
    "#         s=50\n",
    "#     )\n",
    "#     ax.set_xlabel(\"PCA Component 1\")\n",
    "#     ax.set_ylabel(\"PCA Component 2\")\n",
    "#     ax.set_title(f\"Households clustered by {label}\")\n",
    "#     ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afa36a-4cce-4d22-a151-3a6b1f2d961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume:\n",
    "# - weekday[\"pca_coordinates\"] contains PCA results (n_samples x 2)\n",
    "# - weekday[\"labels\"] contains cluster labels\n",
    "# - same for weekend\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, data, label in zip(axs, [weekday, weekend], [\"Weekday\", \"Weekend\"]):\n",
    "    X_pca = data[\"pca_coordinates\"]\n",
    "    clusters = data[\"labels\"]\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        X_pca[:, 0],\n",
    "        X_pca[:, 1],\n",
    "        c=clusters,\n",
    "        cmap='Set1',   # Change color map if desired\n",
    "        s=50\n",
    "    )\n",
    "    ax.set_xlabel(\"PCA Component 1\")\n",
    "    ax.set_ylabel(\"PCA Component 2\")\n",
    "    ax.set_title(f\"Hierarchical Clustering: {label}\")\n",
    "    ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad639860-3daf-4942-98e7-8a9a024fea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume:\n",
    "# - weekday[\"pca_coordinates\"] contains PCA results (n_samples x 2)\n",
    "# - weekday[\"labels\"] contains cluster labels\n",
    "# - same for weekend\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "silhouette_scores = {\"Weekday\": 0.69, \"Weekend\": 0.68}\n",
    "\n",
    "for ax, data, label in zip(axs, [weekday, weekend], [\"Weekday\", \"Weekend\"]):\n",
    "    X_pca = data[\"pca_coordinates\"]\n",
    "    clusters = data[\"labels\"]\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        X_pca[:, 0],\n",
    "        X_pca[:, 1],\n",
    "        c=clusters,\n",
    "        cmap='Set1',\n",
    "        s=50\n",
    "    )\n",
    "    ax.set_xlabel(\"PCA Component 1\")\n",
    "    ax.set_ylabel(\"PCA Component 2\")\n",
    "    ax.set_title(f\"Hierarchical Clustering: {label}\")\n",
    "    ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "\n",
    "    # Add silhouette score to bottom-right corner\n",
    "    score = silhouette_scores[label]\n",
    "    ax.text(\n",
    "        0.95, 0.05,\n",
    "        f\"Silhouette Score = {score:.2f}\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=10,\n",
    "        ha='right',\n",
    "        va='bottom',\n",
    "        bbox=dict(facecolor='white', edgecolor='gray')\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83c17a-0799-490d-806d-5f86875ab422",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_30_rest= {}\n",
    "\n",
    "for house_id in df_3[\"house_hold\"].unique():\n",
    "    house = df_3[df_3[\"house_hold\"] == house_id].copy()\n",
    "    house[\"is_weekday\"] = house[\"is_weekday\"].astype(\"category\")\n",
    "\n",
    "\n",
    "    # Design matrix without intercept\n",
    "    X_interaction_2_separate = dmatrix(\n",
    "        \"C(is_weekday) * season \", data=house, return_type=\"dataframe\"\n",
    "    )\n",
    "\n",
    "\n",
    "    hour_cols_v1 = [f'v1_hour_{h}' for h in range (0,24)]\n",
    "    Y = house[hour_cols_v1].to_numpy()\n",
    "\n",
    "    try:\n",
    "        model_3 = _multivariate_ols_fit(Y, X_interaction_2_separate)\n",
    "        param_30_rest[house_id] = model_3[0]\n",
    "\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipped household {house_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32bb40-b9aa-4f39-8a1e-5ad42b973f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_week_0_rest={}\n",
    "param_week_1_rest={}\n",
    "\n",
    "for house_id in df_3[\"house_hold\"].unique():\n",
    "    house = df_3[df_3[\"house_hold\"] == house_id]\n",
    "\n",
    "    house_0 = house[house[\"is_weekday\"] == 0]\n",
    "    house_1 = house[house[\"is_weekday\"] == 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X_main_effect_2_separate_0 = sm.add_constant(house_0[[\"season\"]])\n",
    "    X_main_effect_2_separate_1 = sm.add_constant(house_1[[\"season\"]])\n",
    "\n",
    "\n",
    "    hour_cols_v1 = [f'v1_hour_{h}' for h in range (0,24)]\n",
    "    Y_0 = house_0[hour_cols_v1].to_numpy()\n",
    "    Y_1 = house_1[hour_cols_v1].to_numpy()\n",
    "    \n",
    "    model_3_0 = _multivariate_ols_fit(Y_0, X_main_effect_2_separate_0)\n",
    "    model_3_1 = _multivariate_ols_fit(Y_1, X_main_effect_2_separate_1)\n",
    "    \n",
    "    param_week_0_rest[house_id]=model_3_0[0]\n",
    "    param_week_1_rest[house_id]=model_3_1[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087e5b4-a9d7-4a95-82d5-9e5dacef8216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_30_rest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690da60-191b-418e-af04-2aad089397f6",
   "metadata": {},
   "source": [
    "## cluster Assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adf278-f139-4b5b-9b0c-dcb9ff6dd78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters_to_new(param_dict_new, scaler, classifier):\n",
    "    \n",
    "    house_ids_new = []\n",
    "    param_vectors_new = []\n",
    "\n",
    "    for house_id, coef_matrix in param_dict_new.items():\n",
    "        flat = coef_matrix.flatten()\n",
    "        param_vectors_new.append(flat)\n",
    "        house_ids_new.append(house_id)\n",
    "\n",
    "    X_new = np.vstack(param_vectors_new)\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "    new_labels = classifier.predict(X_new_scaled)\n",
    "\n",
    "    new_cluster_df = pd.DataFrame({\"house_id\": house_ids_new, \"cluster\": new_labels})\n",
    "    return new_cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c0ce0-4014-4a8b-8cb8-1e7a7445ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_clusters_to_new(param_30_rest, clusters_with_week_weekend[\"scaler\"], clusters_with_week_weekend[\"classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f4feb-47de-4b98-9638-1098eb104de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_clusters_to_new(param_week_0_rest, weekday[\"scaler\"], weekday[\"classifier\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb98b0-57d7-4d27-8787-2d6541e75b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_clusters_to_new(param_week_1_rest, weekend[\"scaler\"], weekend[\"classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab07d9-5fb0-413d-b1fd-b8e3cfbfd89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract mapping Series from weekday and weekend cluster_df\n",
    "house_to_cluster_weekday = weekday[\"cluster_df\"].set_index(\"house_id\")[\"cluster\"]\n",
    "house_to_cluster_weekend = weekend[\"cluster_df\"].set_index(\"house_id\")[\"cluster\"]\n",
    "\n",
    "# Step 2: Map clusters to prediction DataFrames\n",
    "weekday_predict_df[\"cluster\"] = weekday_predict_df[\"house_hold\"].map(house_to_cluster_weekday)\n",
    "weekend_predict_df[\"cluster\"] = weekend_predict_df[\"house_hold\"].map(house_to_cluster_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598babf9-3d20-4c6e-9ef9-9a2665d9a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process each prediction DataFrame\n",
    "def process_predict_df(predict_df):\n",
    "    # Step 1: Melt wide → long format\n",
    "    long_df = predict_df.melt(\n",
    "        id_vars=[\"house_hold\", \"day\", \"cluster\"],\n",
    "        value_vars=[f\"v1_hour_{h}\" for h in range(24)],\n",
    "        var_name=\"hour\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Extract hour number as integer\n",
    "    long_df[\"hour\"] = long_df[\"hour\"].str.extract(r\"v1_hour_(\\d+)\").astype(int)\n",
    "\n",
    "    # Step 3: Compute aggregated average value per cluster/day/hour\n",
    "    group_avg = (\n",
    "        long_df\n",
    "        .groupby([\"cluster\", \"day\", \"hour\"])[\"value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"value\": \"cluster_day_hour_avg\"})\n",
    "    )\n",
    "\n",
    "    # Step 4: Merge average back to long format\n",
    "    long_df_with_avg = long_df.merge(group_avg, on=[\"cluster\", \"day\", \"hour\"])\n",
    "\n",
    "    # Step 5: Group by house_hold, day, hour — keep raw value and cluster mean\n",
    "    household_hourly = (\n",
    "        long_df_with_avg\n",
    "        .groupby([\"house_hold\", \"day\", \"hour\"])\n",
    "        .agg(\n",
    "            raw_value=(\"value\", \"first\"),\n",
    "            cluster=(\"cluster\", \"first\"),\n",
    "            cluster_day_hour_avg=(\"cluster_day_hour_avg\", \"first\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return household_hourly\n",
    "\n",
    "# Apply function to both datasets\n",
    "weekday_hourly = process_predict_df(weekday_predict_df)\n",
    "weekend_hourly = process_predict_df(weekend_predict_df)\n",
    "\n",
    "# # Combine the results (optional)\n",
    "# combined_hourly = pd.concat([weekday_hourly, weekend_hourly], ignore_index=True)\n",
    "\n",
    "# # View sample\n",
    "# print(combined_hourly.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155aa2b4-af31-4c96-8632-c87fd44ab236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_profiles(df, day):\n",
    "    df_day = df[df[\"day\"] == day].copy()\n",
    "    df_day[\"squared_error\"] = (df_day[\"raw_value\"] - df_day[\"cluster_day_hour_avg\"]) ** 2\n",
    "\n",
    "    # Std per cluster/hour\n",
    "    std_df = (\n",
    "        df_day.groupby([\"cluster\", \"hour\"])[\"squared_error\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"squared_error\": \"std\"})\n",
    "    )\n",
    "    std_df[\"std\"] = np.sqrt(std_df[\"std\"])\n",
    "\n",
    "    # Mean per cluster/hour\n",
    "    mean_df = (\n",
    "        df_day.groupby([\"cluster\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "        .first()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge and compute upper/lower bounds\n",
    "    profile_df = mean_df.merge(std_df, on=[\"cluster\", \"hour\"])\n",
    "    profile_df[\"upper\"] = profile_df[\"cluster_day_hour_avg\"] + 3 * profile_df[\"std\"]\n",
    "    profile_df[\"lower\"] = profile_df[\"cluster_day_hour_avg\"] - 3 * profile_df[\"std\"]\n",
    "\n",
    "    return profile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b504fc-35bd-447c-8cc3-089c01a34310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_cluster_profiles(profile_df, title_prefix=\"\", day=None):\n",
    "    clusters = sorted(profile_df[\"cluster\"].unique())\n",
    "    n_clusters = len(clusters)\n",
    "    n_rows = 2\n",
    "    n_cols = 2\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(14, 8), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()  # Flatten to index like a list\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        subset = profile_df[profile_df[\"cluster\"] == cluster]\n",
    "        ax = axs[i]\n",
    "\n",
    "        ax.plot(subset[\"hour\"], subset[\"cluster_day_hour_avg\"], label=\"Mean Load\", linewidth=2)\n",
    "        ax.fill_between(subset[\"hour\"], subset[\"lower\"], subset[\"upper\"], alpha=0.3, label=\"±3σ Range\")\n",
    "        ax.set_title(f\"{title_prefix}Cluster {cluster}\" + (f\" (Day {day})\" if day else \"\"))\n",
    "        ax.set_xlabel(\"Hour\")\n",
    "        ax.set_ylabel(\"Load\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    # Hide unused subplots if any\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c170d-63ab-473a-af20-debc090fe34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_hourly = process_predict_df(weekday_predict_df)\n",
    "weekend_hourly = process_predict_df(weekend_predict_df)\n",
    "\n",
    "\n",
    "\n",
    "# Generate and plot weekday\n",
    "weekday_profile = compute_cluster_profiles(weekday_hourly, 1)\n",
    "plot_cluster_profiles(weekday_profile, title_prefix=\"Weekday \", day=1)\n",
    "\n",
    "# Generate and plot weekend\n",
    "weekend_profile = compute_cluster_profiles(weekend_hourly, 5)\n",
    "plot_cluster_profiles(weekend_profile, title_prefix=\"Weekend \", day=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62c9a3-cb38-4ec2-be3d-3e9861152c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_3[df_3[\"house_hold\"]==8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261593eb-8f0c-4f6b-9043-59dd565c2522",
   "metadata": {},
   "source": [
    "## kalman fulter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ef3df-6a98-49ff-b056-2d83ed78d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanForecaster:\n",
    "    def __init__(self, Theta_X=0.8, Theta_v=0.2, theta_z=1.0, Q_diag=[1, 1, 1], R_diag=[10, 10, 10], P0=1.0):\n",
    "        self.Theta_X = Theta_X\n",
    "        self.Theta_v = Theta_v\n",
    "        self.theta_z = theta_z\n",
    "        self.Q_diag = Q_diag\n",
    "        self.R_diag = R_diag\n",
    "        self.P0 = P0\n",
    "        self.n_vars = 3  # For 3 households\n",
    "\n",
    "        # Core matrices (defined at fit time)\n",
    "        self.F = np.eye(self.n_vars) * self.Theta_X\n",
    "        self.B = np.eye(self.n_vars) * self.Theta_v\n",
    "        self.H = np.eye(self.n_vars) * self.theta_z\n",
    "        self.Q = np.diag(self.Q_diag)\n",
    "        self.R = np.diag(self.R_diag)\n",
    "\n",
    "    def fit(self, X_train, V_train=None):\n",
    "        X = X_train[0, :]\n",
    "        P = np.diag([self.P0] * self.n_vars)\n",
    "        filtered_states = []\n",
    "\n",
    "        for t in range(X_train.shape[0]):\n",
    "            z = X_train[t]\n",
    "            v = V_train[t] if V_train is not None and not isinstance(V_train, int) else np.zeros(self.n_vars)\n",
    "\n",
    "            # Predict\n",
    "            X_pred = self.F @ X + self.B @ v\n",
    "            P_pred = self.F @ P @ self.F.T * self.Q\n",
    "\n",
    "            # Update\n",
    "            S = self.H @ P_pred @ self.H.T + self.R\n",
    "            K = P_pred @ self.H.T @ np.linalg.inv(S)\n",
    "            X = X_pred + K @ (z - self.H @ X_pred)\n",
    "            P = (np.eye(self.n_vars) - K @ self.H) @ P_pred\n",
    "\n",
    "            filtered_states.append(X.copy())\n",
    "\n",
    "        self.last_X = X  # for forecasting\n",
    "        self.last_P = P\n",
    "\n",
    "        return pd.DataFrame(filtered_states, columns=[\"Filtered_8\", \"Filtered_11\", \"Filtered_34\"])\n",
    "\n",
    "    def forecast(self, V_test=None, steps=None):\n",
    "        assert hasattr(self, 'last_X'), \"You must call .fit() before .forecast()\"\n",
    "        X = self.last_X.copy()\n",
    "        P = self.last_P.copy()\n",
    "        forecasts = []\n",
    "\n",
    "        if isinstance(V_test, int) or V_test is None:\n",
    "            V_test = np.zeros((steps, self.n_vars))\n",
    "\n",
    "        for t in range(V_test.shape[0]):\n",
    "            v = V_test[t]\n",
    "            X = self.F @ X + self.B @ v\n",
    "            forecasts.append(X.copy())\n",
    "\n",
    "        return pd.DataFrame(forecasts, columns=[\"Forecast_8\", \"Forecast_11\", \"Forecast_34\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551d150-a99b-4e10-9b42-555b6e56b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def multivariate_kalman_loss_per_household_qr(\n",
    "    params,\n",
    "    X_train,\n",
    "    V_train=None,\n",
    "    n_folds=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Cross-validation loss for a multivariate Kalman filter with\n",
    "    household-specific Q and R. If V_train is None, uses zeros.\n",
    "\n",
    "    Parameters:\n",
    "        params (list): [\n",
    "            Theta_X, Theta_v, theta_z,\n",
    "            q_8, q_11, q_34,\n",
    "            r_8, r_11, r_34\n",
    "        ]\n",
    "        X_train (ndarray): observed data (n_timesteps, 3)\n",
    "        V_train (ndarray or None): load-profile input (n_timesteps, 3)\n",
    "        n_folds (int): number of CV folds\n",
    "\n",
    "    Returns:\n",
    "        float: average RMSE across households and folds\n",
    "    \"\"\"\n",
    "    # Unpack the 9 parameters\n",
    "    Theta_X, Theta_v, theta_z = params[:3]\n",
    "    Q_diag = params[3:6]\n",
    "    R_diag = params[6:9]\n",
    "\n",
    "    # If no exogenous input provided, use zeros\n",
    "    if V_train is None:\n",
    "        V_train = np.zeros_like(X_train)\n",
    "\n",
    "    n = len(X_train)\n",
    "    fold_size = n // n_folds\n",
    "    rmse_scores = []\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        start = fold * fold_size\n",
    "        end = (fold + 1) * fold_size if fold < n_folds - 1 else n\n",
    "\n",
    "        # Validation slice\n",
    "        X_val = X_train[start:end]\n",
    "        V_val = V_train[start:end]\n",
    "\n",
    "        # Sub-training slice (everything except [start:end])\n",
    "        X_sub = np.concatenate((X_train[:start], X_train[end:]), axis=0)\n",
    "        V_sub = np.concatenate((V_train[:start], V_train[end:]), axis=0)\n",
    "\n",
    "        # Build & fit the Kalman filter\n",
    "        kf = KalmanForecaster(\n",
    "            Theta_X=Theta_X,\n",
    "            Theta_v=Theta_v,\n",
    "            theta_z=theta_z,\n",
    "            Q_diag=Q_diag,\n",
    "            R_diag=R_diag\n",
    "        )\n",
    "        kf.fit(X_sub, V_sub)\n",
    "\n",
    "        # Forecast on the held-out block\n",
    "        forecast_df = kf.forecast(V_val)\n",
    "\n",
    "        # Compute per-household RMSE\n",
    "        rmse_8  = mean_squared_error(X_val[:, 0], forecast_df[\"Forecast_8\"], squared=False)\n",
    "        rmse_11 = mean_squared_error(X_val[:, 1], forecast_df[\"Forecast_11\"], squared=False)\n",
    "        rmse_34 = mean_squared_error(X_val[:, 2], forecast_df[\"Forecast_34\"], squared=False)\n",
    "\n",
    "        rmse_scores.append((rmse_8 + rmse_11 + rmse_34) / 3)\n",
    "\n",
    "    return np.mean(rmse_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59324b-4706-49db-be43-dabaed73c275",
   "metadata": {},
   "source": [
    "## weeked kalman v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f673d-f4f5-43e6-98c8-145b5b0e8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_0 = weekend_hourly[weekend_hourly[\"cluster\"] == 0]\n",
    "\n",
    "load_profile_matrix = (\n",
    "    cluster_0.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "load_profile_flat = load_profile_matrix.values.flatten()\n",
    "\n",
    "available_days = load_profile_matrix.index.tolist()\n",
    "df_filtered = df_3[\n",
    "    df_3[\"day\"].isin(available_days) &\n",
    "    df_3[\"house_hold\"].isin([8, 11, 34])\n",
    "].sort_values(by=[\"house_hold\", \"day\"]).reset_index(drop=True)\n",
    "\n",
    "hour_cols = [f\"v1_hour_{i}\" for i in range(24)]\n",
    "households = [8, 11, 34]\n",
    "observed_matrix = np.vstack([\n",
    "    df_filtered[df_filtered[\"house_hold\"] == h][hour_cols].values.flatten()\n",
    "    for h in households\n",
    "]).T\n",
    "\n",
    "V_matrix = np.repeat(load_profile_flat.reshape(-1, 1), 3, axis=1)\n",
    "\n",
    "split_point = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50 = observed_matrix[:split_point]\n",
    "X_test_50 = observed_matrix[split_point:]\n",
    "V_train_50 = V_matrix[:split_point]\n",
    "V_test_50 = V_matrix[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b321232-a4c2-4eaa-ba98-33de40a3a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e30be-2f5c-4f67-adf5-42198ad5b8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797d4c7-ac8e-4c77-8a9f-d95897d3cfd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Number of random trials\n",
    "n_trials = 200\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Sample parameters uniformly from your desired ranges\n",
    "    Theta_X  = np.random.uniform(0.1, 0.99)\n",
    "    Theta_v  = np.random.uniform(0.01, 1.0)\n",
    "    theta_z  = np.random.uniform(0.1,  2.0)\n",
    "    q8  = np.random.uniform(0.01, 10.0)\n",
    "    q11 = np.random.uniform(0.01, 10.0)\n",
    "    q34 = np.random.uniform(0.01, 10.0)\n",
    "    r8  = np.random.uniform(0.1,  50.0)\n",
    "    r11 = np.random.uniform(0.1,  50.0)\n",
    "    r34 = np.random.uniform(0.1,  50.0)\n",
    "\n",
    "    params = [Theta_X, Theta_v, theta_z, q8, q11, q34, r8, r11, r34]\n",
    "    loss = multivariate_kalman_loss_per_household_qr(params, X_train_50, V_train_50, n_folds=3)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_params = params\n",
    "\n",
    "    print(f\"Trial {trial+1}/{n_trials}: RMSE = {loss:.2f}\")\n",
    "\n",
    "print(\"\\n🎯 Best parameters found:\")\n",
    "print(f\" Θₓ  = {best_params[0]:.4f}\")\n",
    "print(f\" Θᵥ  = {best_params[1]:.4f}\")\n",
    "print(f\" θ_z = {best_params[2]:.4f}\")\n",
    "print(f\" q₈  = {best_params[3]:.4f}, q₁₁ = {best_params[4]:.4f}, q₃₄ = {best_params[5]:.4f}\")\n",
    "print(f\" r₈  = {best_params[6]:.4f}, r₁₁ = {best_params[7]:.4f}, r₃₄ = {best_params[8]:.4f}\")\n",
    "print(f\"Best cross‐validated RMSE: {best_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fc805-3e12-44b0-8e57-65b98b6152ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# === Fit Kalman Filter on Training ===\n",
    "kf = KalmanForecaster(\n",
    "    Theta_X= 0.3550,\n",
    "    Theta_v=0.5949,\n",
    "    theta_z= 0.1580,\n",
    "    Q_diag=[0.3831,8.2278, 3.6083],\n",
    "    R_diag=[6.4403, 26.1599, 38.5227]\n",
    ")\n",
    "\n",
    "filtered_df = kf.fit(X_train_50, V_train_50)\n",
    "forecast_df = kf.forecast(V_test_50)\n",
    "\n",
    "# === Compute RMSE ===\n",
    "rmse_fit_8 = mean_squared_error(X_train_50[:, 0], filtered_df[\"Filtered_8\"], squared=False)\n",
    "rmse_fit_11 = mean_squared_error(X_train_50[:, 1], filtered_df[\"Filtered_11\"], squared=False)\n",
    "rmse_fit_34 = mean_squared_error(X_train_50[:, 2], filtered_df[\"Filtered_34\"], squared=False)\n",
    "\n",
    "rmse_fore_8 = mean_squared_error(X_test_50[:, 0], forecast_df[\"Forecast_8\"], squared=False)\n",
    "rmse_fore_11 = mean_squared_error(X_test_50[:, 1], forecast_df[\"Forecast_11\"], squared=False)\n",
    "rmse_fore_34 = mean_squared_error(X_test_50[:, 2], forecast_df[\"Forecast_34\"], squared=False)\n",
    "\n",
    "# === Plot: Fitted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_train_50[:, 0], label=\"True 8\")\n",
    "plt.plot(filtered_df[\"Filtered_8\"], label=f\"Fitted 8 (RMSE={rmse_fit_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_train_50[:, 1], label=\"True 11\")\n",
    "plt.plot(filtered_df[\"Filtered_11\"], label=f\"Fitted 11 (RMSE={rmse_fit_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_train_50[:, 2], label=\"True 34\")\n",
    "plt.plot(filtered_df[\"Filtered_34\"], label=f\"Fitted 34 (RMSE={rmse_fit_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot: Forecasted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_test_50[:, 0], label=\"True 8\")\n",
    "plt.plot(forecast_df[\"Forecast_8\"], label=f\"Forecast 8 (RMSE={rmse_fore_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_test_50[:, 1], label=\"True 11\")\n",
    "plt.plot(forecast_df[\"Forecast_11\"], label=f\"Forecast 11 (RMSE={rmse_fore_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_test_50[:, 2], label=\"True 34\")\n",
    "plt.plot(forecast_df[\"Forecast_34\"], label=f\"Forecast 34 (RMSE={rmse_fore_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906fa16-2477-46b8-baf3-f1e7c7482439",
   "metadata": {},
   "source": [
    "## weekday Kalman v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4186cac-9b3f-447e-90b5-9a3e2161b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = weekday_hourly[weekday_hourly[\"cluster\"] == 0]\n",
    "\n",
    "load_profile_matrix = (\n",
    "    cluster_0.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "load_profile_flat = load_profile_matrix.values.flatten()\n",
    "\n",
    "available_days = load_profile_matrix.index.tolist()\n",
    "df_filtered = df_3[\n",
    "    df_3[\"day\"].isin(available_days) &\n",
    "    df_3[\"house_hold\"].isin([8, 11, 34])\n",
    "].sort_values(by=[\"house_hold\", \"day\"]).reset_index(drop=True)\n",
    "\n",
    "hour_cols = [f\"v1_hour_{i}\" for i in range(24)]\n",
    "households = [8, 11, 34]\n",
    "observed_matrix = np.vstack([\n",
    "    df_filtered[df_filtered[\"house_hold\"] == h][hour_cols].values.flatten()\n",
    "    for h in households\n",
    "]).T\n",
    "\n",
    "V_matrix = np.repeat(load_profile_flat.reshape(-1, 1), 3, axis=1)\n",
    "\n",
    "split_point_week = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50_week  = observed_matrix[:split_point]\n",
    "X_test_50_week = observed_matrix[split_point:]\n",
    "V_train_50_week  = V_matrix[:split_point]\n",
    "V_test_50_week = V_matrix[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf7151-4511-4c8d-81c8-57d4acdff88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(43)\n",
    "random.seed(43)\n",
    "# Number of random trials\n",
    "n_trials = 200\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Sample parameters uniformly from your desired ranges\n",
    "    Theta_X  = np.random.uniform(0.1, 0.99)\n",
    "    Theta_v  = np.random.uniform(0.01, 1.0)\n",
    "    theta_z  = np.random.uniform(0.1,  2.0)\n",
    "    q8  = np.random.uniform(0.01, 10.0)\n",
    "    q11 = np.random.uniform(0.01, 10.0)\n",
    "    q34 = np.random.uniform(0.01, 10.0)\n",
    "    r8  = np.random.uniform(0.1,  50.0)\n",
    "    r11 = np.random.uniform(0.1,  50.0)\n",
    "    r34 = np.random.uniform(0.1,  50.0)\n",
    "\n",
    "    params = [Theta_X, Theta_v, theta_z, q8, q11, q34, r8, r11, r34]\n",
    "    loss = multivariate_kalman_loss_per_household_qr(params, X_train_50_week, V_train_50_week, n_folds=3)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_params = params\n",
    "\n",
    "    print(f\"Trial {trial+1}/{n_trials}: RMSE = {loss:.2f}\")\n",
    "\n",
    "print(\"\\n🎯 Best parameters found:\")\n",
    "print(f\" Θₓ  = {best_params[0]:.4f}\")\n",
    "print(f\" Θᵥ  = {best_params[1]:.4f}\")\n",
    "print(f\" θ_z = {best_params[2]:.4f}\")\n",
    "print(f\" q₈  = {best_params[3]:.4f}, q₁₁ = {best_params[4]:.4f}, q₃₄ = {best_params[5]:.4f}\")\n",
    "print(f\" r₈  = {best_params[6]:.4f}, r₁₁ = {best_params[7]:.4f}, r₃₄ = {best_params[8]:.4f}\")\n",
    "print(f\"Best cross‐validated RMSE: {best_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2309d3-7ce8-4e86-a80e-84dccce1b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KalmanForecaster(\n",
    "    Theta_X=0.4970,\n",
    "    Theta_v=0.4308,\n",
    "    theta_z= 0.6257,\n",
    "    Q_diag=[9.4448,7.3696, 9.9191],\n",
    "    R_diag=[25.9738, 35.0268,  21.8945]\n",
    ")\n",
    "\n",
    "filtered_df_week = kf.fit(X_train_50_week, V_train_50_week)\n",
    "forecast_df_week = kf.forecast(V_test_50_week)\n",
    "\n",
    "# === Compute RMSE ===\n",
    "rmse_fit_8 = mean_squared_error(X_train_50_week[:, 0], filtered_df_week[\"Filtered_8\"], squared=False)\n",
    "rmse_fit_11 = mean_squared_error(X_train_50_week[:, 1], filtered_df_week[\"Filtered_11\"], squared=False)\n",
    "rmse_fit_34 = mean_squared_error(X_train_50_week[:, 2], filtered_df_week[\"Filtered_34\"], squared=False)\n",
    "\n",
    "rmse_fore_8 = mean_squared_error(X_test_50_week[:, 0], forecast_df_week[\"Forecast_8\"], squared=False)\n",
    "rmse_fore_11 = mean_squared_error(X_test_50_week[:, 1], forecast_df_week[\"Forecast_11\"], squared=False)\n",
    "rmse_fore_34 = mean_squared_error(X_test_50_week[:, 2], forecast_df_week[\"Forecast_34\"], squared=False)\n",
    "\n",
    "# === Plot: Fitted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_train_50_week[:, 0], label=\"True 8\")\n",
    "plt.plot(filtered_df_week[\"Filtered_8\"], label=f\"Fitted 8 (RMSE={rmse_fit_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_train_50_week[:, 1], label=\"True 11\")\n",
    "plt.plot(filtered_df_week[\"Filtered_11\"], label=f\"Fitted 11 (RMSE={rmse_fit_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_train_50_week[:, 2], label=\"True 34\")\n",
    "plt.plot(filtered_df_week[\"Filtered_34\"], label=f\"Fitted 34 (RMSE={rmse_fit_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot: Forecasted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_test_50_week[:, 0], label=\"True 8\")\n",
    "plt.plot(forecast_df_week[\"Forecast_8\"], label=f\"Forecast 8 (RMSE={rmse_fore_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_test_50_week[:, 1], label=\"True 11\")\n",
    "plt.plot(forecast_df_week[\"Forecast_11\"], label=f\"Forecast 11 (RMSE={rmse_fore_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_test_50_week[:, 2], label=\"True 34\")\n",
    "plt.plot(forecast_df_week[\"Forecast_34\"], label=f\"Forecast 34 (RMSE={rmse_fore_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023095a-13a1-4547-9971-f03f7765e8d1",
   "metadata": {},
   "source": [
    "\n",
    "## without load v1 kalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3d78f-11c0-40c9-9db0-479a46e2627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Define the hourly columns and target households\n",
    "hour_columns = [f\"v1_hour_{i}\" for i in range(24)]\n",
    "households   = [8, 11, 34]\n",
    "\n",
    "# 2) For each household, sort by day and flatten its 365×24 values into one long series\n",
    "series_list = []\n",
    "for hh in households:\n",
    "    hh_df = df_3[df_3[\"house_hold\"] == hh].sort_values(\"day\")\n",
    "    # ensure 365 days × 24 hours\n",
    "    assert hh_df.shape[0] == 365, f\"Expected 365 days for HH {hh}, got {hh_df.shape[0]}\"\n",
    "    flat = hh_df[hour_columns].to_numpy().reshape(-1)  # length = 365*24\n",
    "    series_list.append(flat)\n",
    "\n",
    "# 3) Stack into an observed‐data matrix of shape (8760, 3)\n",
    "observed_matrix = np.vstack(series_list).T\n",
    "\n",
    "split_point_without_load = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50_without_load  = observed_matrix[:split_point]\n",
    "X_test_50_without_load = observed_matrix[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ce533-01e3-48b3-92c1-b15ecec1dfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Number of random trials\n",
    "n_trials = 200\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Sample parameters uniformly from your desired ranges\n",
    "    Theta_X  = np.random.uniform(0.1, 0.99)\n",
    "    Theta_v  = np.random.uniform(0.01, 1.0)\n",
    "    theta_z  = np.random.uniform(0.1,  2.0)\n",
    "    q8  = np.random.uniform(0.01, 10.0)\n",
    "    q11 = np.random.uniform(0.01, 10.0)\n",
    "    q34 = np.random.uniform(0.01, 10.0)\n",
    "    r8  = np.random.uniform(0.1,  50.0)\n",
    "    r11 = np.random.uniform(0.1,  50.0)\n",
    "    r34 = np.random.uniform(0.1,  50.0)\n",
    "\n",
    "    params = [Theta_X, Theta_v, theta_z, q8, q11, q34, r8, r11, r34]\n",
    "    loss = multivariate_kalman_loss_per_household_qr(params, X_train_50_without_load, n_folds=3)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_params = params\n",
    "\n",
    "    print(f\"Trial {trial+1}/{n_trials}: RMSE = {loss:.2f}\")\n",
    "\n",
    "print(\"\\n🎯 Best parameters found:\")\n",
    "print(f\" Θₓ  = {best_params[0]:.4f}\")\n",
    "print(f\" Θᵥ  = {best_params[1]:.4f}\")\n",
    "print(f\" θ_z = {best_params[2]:.4f}\")\n",
    "print(f\" q₈  = {best_params[3]:.4f}, q₁₁ = {best_params[4]:.4f}, q₃₄ = {best_params[5]:.4f}\")\n",
    "print(f\" r₈  = {best_params[6]:.4f}, r₁₁ = {best_params[7]:.4f}, r₃₄ = {best_params[8]:.4f}\")\n",
    "print(f\"Best cross‐validated RMSE: {best_loss:.2f}\"  \"\"\"\n",
    "    Random search tuning for your ParticleFilter.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (ndarray): observed input (T, D)\n",
    "        V_train (ndarray or None): exogenous input (T, D)\n",
    "        V_future (ndarray or None): future V for prediction (steps, D)\n",
    "        y_true (ndarray): ground truth for future (steps, D) — used for RMSE\n",
    "        ParticleClass (class): your ParticleFilter class\n",
    "        n_trials (int): number of random trials\n",
    "        num_particles (int): number of particles in the filter\n",
    "        seed (int): random seed\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_loss, best_params)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83429bf2-b1fe-49d2-b036-7bb205569fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fit Kalman Filter on Training ===\n",
    "kf = KalmanForecaster(\n",
    "    Theta_X= 0.9890,\n",
    "    Theta_v=0.8846,\n",
    "    theta_z= 0.8312,\n",
    "    Q_diag=[2.5687,4.2573, 1.1747],\n",
    "    R_diag=[32.4325, 6.2845, 13.1311]\n",
    ")\n",
    "\n",
    "filtered_df_without_load = kf.fit(X_train_50_without_load)\n",
    "forecast_df_without_load = kf.forecast(steps=X_test_50_without_load.shape[0])\n",
    "\n",
    "# === Compute RMSE ===\n",
    "rmse_fit_8 = mean_squared_error(X_train_50_without_load[:, 0], filtered_df_without_load[\"Filtered_8\"], squared=False)\n",
    "rmse_fit_11 = mean_squared_error(X_train_50_without_load[:, 1], filtered_df_without_load[\"Filtered_11\"], squared=False)\n",
    "rmse_fit_34 = mean_squared_error(X_train_50_without_load[:, 2], filtered_df_without_load[\"Filtered_34\"], squared=False)\n",
    "\n",
    "rmse_fore_8 = mean_squared_error(X_test_50_without_load[:, 0], forecast_df_without_load[\"Forecast_8\"], squared=False)\n",
    "rmse_fore_11 = mean_squared_error(X_test_50_without_load[:, 1], forecast_df_without_load[\"Forecast_11\"], squared=False)\n",
    "rmse_fore_34 = mean_squared_error(X_test_50_without_load[:, 2], forecast_df_without_load[\"Forecast_34\"], squared=False)\n",
    "\n",
    "# === Plot: Fitted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_train_50_without_load[:, 0], label=\"True 8\")\n",
    "plt.plot(filtered_df_without_load[\"Filtered_8\"], label=f\"Fitted 8 (RMSE={rmse_fit_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_train_50_without_load[:, 1], label=\"True 11\")\n",
    "plt.plot(filtered_df_without_load[\"Filtered_11\"], label=f\"Fitted 11 (RMSE={rmse_fit_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_train_50_without_load[:, 2], label=\"True 34\")\n",
    "plt.plot(filtered_df_without_load[\"Filtered_34\"], label=f\"Fitted 34 (RMSE={rmse_fit_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot: Forecasted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_test_50_without_load[:, 0], label=\"True 8\")\n",
    "plt.plot(forecast_df_without_load[\"Forecast_8\"], label=f\"Forecast 8 (RMSE={rmse_fore_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_test_50_without_load[:, 1], label=\"True 11\")\n",
    "plt.plot(forecast_df_without_load[\"Forecast_11\"], label=f\"Forecast 11 (RMSE={rmse_fore_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_test_50_without_load[:, 2], label=\"True 34\")\n",
    "plt.plot(forecast_df_without_load[\"Forecast_34\"], label=f\"Forecast 34 (RMSE={rmse_fore_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fddbd4-9383-409e-8832-e8f42da84583",
   "metadata": {},
   "source": [
    "## particle_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49d486-b85f-4237-9376-3060328efac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ParticleFilter:\n",
    "    def __init__(self,\n",
    "                 num_particles=500,\n",
    "                 theta_x=0.8,\n",
    "                 theta_v=0.2,\n",
    "                 theta_z=1.0,\n",
    "                 process_noise_std=5.0,\n",
    "                 measurement_noise_std=10.0):\n",
    "        \"\"\"\n",
    "        A particle filter with optional exogenous input V and a linear measurement model z = θ_z * x + noise.\n",
    "        \"\"\"\n",
    "        self.M = num_particles\n",
    "        self.theta_x = theta_x\n",
    "        self.theta_v = theta_v\n",
    "        self.theta_z = theta_z\n",
    "        self.q_std = process_noise_std\n",
    "        self.r_std = measurement_noise_std\n",
    "\n",
    "        self.particles = None    # shape (M, D)\n",
    "        self.weights = None      # shape (M,)\n",
    "        self.filtered = None     # DataFrame of filtered means\n",
    "\n",
    "    def fit(self, X_obs, V_matrix=None):\n",
    "        \"\"\"\n",
    "        Run the filter over observed data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_obs : ndarray, shape (T, D)\n",
    "            Observed time‑series.\n",
    "        V_matrix : ndarray, shape (T, D), optional\n",
    "            Exogenous input; if None, filter runs without it.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Columns = ['Hour', 'Observed_0', …, 'Observed_{D-1}', 'Filtered_0', …, 'Filtered_{D-1}']\n",
    "        \"\"\"\n",
    "        T, D = X_obs.shape\n",
    "        # 1) initialize particles around the scale of the first observations\n",
    "        self.particles = np.random.randn(self.M, D) * np.std(X_obs, axis=0)\n",
    "        self.weights = np.ones(self.M) / self.M\n",
    "\n",
    "        means = np.zeros((T, D))\n",
    "        \n",
    "        for t in range(T):\n",
    "            # 2) Propagation step\n",
    "            noise = np.random.randn(self.M, D) * self.q_std\n",
    "            if V_matrix is not None:\n",
    "                self.particles = (\n",
    "                    self.theta_x * self.particles\n",
    "                    + self.theta_v * V_matrix[t]\n",
    "                    + noise\n",
    "                )\n",
    "            else:\n",
    "                self.particles = (\n",
    "                    self.theta_x * self.particles\n",
    "                    + noise\n",
    "                )\n",
    "            \n",
    "            # 3) Weighting step using z_t = theta_z * x_t + measurement noise\n",
    "            obs = X_obs[t]  # shape (D,)\n",
    "            # predicted measurements for each particle\n",
    "            predicted_z = self.theta_z * self.particles      # (M, D)\n",
    "            diffs = obs[None, :] - predicted_z               # (M, D)\n",
    "            exponent = -0.5 * np.sum((diffs / self.r_std)**2, axis=1)\n",
    "            coeff = (1 / np.sqrt(2 * np.pi * self.r_std**2))**D\n",
    "            likelihood = coeff * np.exp(exponent)\n",
    "            self.weights *= likelihood\n",
    "            self.weights += 1e-300     # avoid zeros\n",
    "            self.weights /= np.sum(self.weights)\n",
    "\n",
    "            # 4) Estimation step: weighted mean of the state‐particles\n",
    "            means[t] = np.average(self.particles, axis=0, weights=self.weights)\n",
    "\n",
    "            # 5) Resampling step (multinomial)\n",
    "            idx = np.random.choice(self.M, size=self.M, p=self.weights)\n",
    "            self.particles = self.particles[idx]\n",
    "            self.weights.fill(1.0 / self.M)\n",
    "\n",
    "        # Package results into a DataFrame\n",
    "        df = pd.DataFrame(means,\n",
    "                          columns=[f\"Filtered_{i}\" for i in range(D)])\n",
    "        df.insert(0, \"Hour\", np.arange(T))\n",
    "        for i in range(D):\n",
    "            df[f\"Observed_{i}\"] = X_obs[:, i]\n",
    "        self.filtered = df\n",
    "        return df\n",
    "\n",
    "    def predict(self, V_future=None, steps=1):\n",
    "        \"\"\"\n",
    "        Forecast future states via pure propagation (no measurement update).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        V_future : ndarray, shape (steps, D), optional\n",
    "            Future exogenous input. If None, predict without it.\n",
    "        steps : int\n",
    "            Number of future steps to generate.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Columns = ['Step', 'Predicted_0', …, 'Predicted_{D-1}']\n",
    "        \"\"\"\n",
    "        D = self.particles.shape[1]\n",
    "        preds = np.zeros((steps, D))\n",
    "\n",
    "        for t in range(steps):\n",
    "            noise = np.random.randn(self.M, D) * self.q_std\n",
    "            if V_future is not None:\n",
    "                self.particles = (\n",
    "                    self.theta_x * self.particles\n",
    "                    + self.theta_v * V_future[t]\n",
    "                    + noise\n",
    "                )\n",
    "            else:\n",
    "                self.particles = (\n",
    "                    self.theta_x * self.particles\n",
    "                    + noise\n",
    "                )\n",
    "            preds[t] = np.mean(self.particles, axis=0)\n",
    "\n",
    "        df = pd.DataFrame(preds,\n",
    "                          columns=[f\"Predicted_{i}\" for i in range(D)])\n",
    "        df.insert(0, \"Step\", np.arange(1, steps+1))\n",
    "        return df\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4883dd-2668-41c4-acdb-944b8c7e2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def combined_filter_loss(\n",
    "    params,\n",
    "    X_train,\n",
    "    V_train=None,\n",
    "    n_folds=2,\n",
    "    method=\"kalman\",  # or \"particle\"\n",
    "    KalmanClass=None,  # class for KalmanFilter\n",
    "    ParticleClass=None  # class for ParticleFilter\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute CV loss for either Kalman or Particle Filter.\n",
    "\n",
    "    Parameters:\n",
    "        params (list): [theta_x, theta_v, theta_z, q_8, q_11, q_34, r_8, r_11, r_34]\n",
    "        X_train (ndarray): shape (T, 3), observed data\n",
    "        V_train (ndarray or None): shape (T, 3), exogenous inputs\n",
    "        n_folds (int): number of folds\n",
    "        method (str): \"kalman\" or \"particle\"\n",
    "        KalmanClass (class): Kalman filter class (must implement fit & forecast)\n",
    "        ParticleClass (class): Particle filter class (must implement fit & predict)\n",
    "\n",
    "    Returns:\n",
    "        float: average RMSE across households and folds\n",
    "    \"\"\"\n",
    "    theta_x, theta_v, theta_z = params[:3]\n",
    "    q_values = params[3:6]\n",
    "    r_values = params[6:9]\n",
    "\n",
    "    if V_train is None:\n",
    "        V_train = np.zeros_like(X_train)\n",
    "\n",
    "    T = len(X_train)\n",
    "    fold_size = T // n_folds\n",
    "    rmse_scores = []\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        start = fold * fold_size\n",
    "        end = (fold + 1) * fold_size if fold < n_folds - 1 else T\n",
    "\n",
    "        X_val = X_train[start:end]\n",
    "        V_val = V_train[start:end]\n",
    "        X_sub = np.concatenate((X_train[:start], X_train[end:]), axis=0)\n",
    "        V_sub = np.concatenate((V_train[:start], V_train[end:]), axis=0)\n",
    "\n",
    "        rmses = []\n",
    "\n",
    "        for i, (q, r) in enumerate(zip(q_values, r_values)):\n",
    "            if method == \"kalman\":\n",
    "                kf = KalmanClass(\n",
    "                    Theta_X=theta_x,\n",
    "                    Theta_v=theta_v,\n",
    "                    theta_z=theta_z,\n",
    "                    Q_diag=[q],\n",
    "                    R_diag=[r]\n",
    "                )\n",
    "                kf.fit(X_sub[:, [i]], V_sub[:, [i]])\n",
    "                forecast_df = kf.forecast(V_val[:, [i]])\n",
    "                pred = forecast_df[f\"Forecast_{[8,11,34][i]}\"]\n",
    "\n",
    "            elif method == \"particle\":\n",
    "                pf = ParticleClass(\n",
    "                    num_particles=500,\n",
    "                    theta_x=theta_x,\n",
    "                    theta_v=theta_v,\n",
    "                    theta_z=theta_z,\n",
    "                    process_noise_std=q,\n",
    "                    measurement_noise_std=r\n",
    "                )\n",
    "                pf.fit(X_sub[:, [i]], V_sub[:, [i]])\n",
    "                forecast_df = pf.predict(V_future=V_val[:, [i]], steps=len(X_val))\n",
    "                pred = forecast_df[\"Predicted_0\"]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"method must be 'kalman' or 'particle'\")\n",
    "\n",
    "            rmse = mean_squared_error(X_val[:, i], pred, squared=False)\n",
    "            rmses.append(rmse)\n",
    "\n",
    "        rmse_scores.append(np.mean(rmses))\n",
    "\n",
    "    return np.mean(rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca47a9-7e09-450c-9aeb-e9dd5a7f2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "\n",
    "def tune_particle_filter_on_training(\n",
    "    X_train,\n",
    "    V_train=None,\n",
    "    ParticleClass=None,\n",
    "    n_trials=100,\n",
    "    num_particles=500,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Tune Particle Filter parameters using training error only (filtered vs observed).\n",
    "\n",
    "    Parameters:\n",
    "        X_train (ndarray): shape (T, D), observed data\n",
    "        V_train (ndarray or None): shape (T, D), exogenous input (optional)\n",
    "        ParticleClass (class): your ParticleFilter class\n",
    "        n_trials (int): number of random configurations to try\n",
    "        num_particles (int): number of particles\n",
    "        seed (int): random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        (best_loss, best_params)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # --- Random parameter sampling ---\n",
    "        theta_x = np.random.uniform(0.5, 0.99)\n",
    "        theta_v = np.random.uniform(0.0, 1.0)\n",
    "        theta_z = np.random.uniform(0.5, 2.0)\n",
    "        q_std   = np.random.uniform(0.01, 10.0)\n",
    "        r_std   = np.random.uniform(0.1, 50.0)\n",
    "\n",
    "        # --- Initialize and fit filter ---\n",
    "        pf = ParticleClass(\n",
    "            num_particles=num_particles,\n",
    "            theta_x=theta_x,\n",
    "            theta_v=theta_v,\n",
    "            theta_z=theta_z,\n",
    "            process_noise_std=q_std,\n",
    "            measurement_noise_std=r_std\n",
    "        )\n",
    "\n",
    "        pf.fit(X_train, V_matrix=V_train)\n",
    "        filtered = pf.filtered[[col for col in pf.filtered.columns if col.startswith(\"Filtered_\")]].values\n",
    "\n",
    "        # --- Compute RMSE between filtered states and original observed data ---\n",
    "        loss = mean_squared_error(X_train, filtered, squared=False)\n",
    "\n",
    "        print(f\"Trial {trial+1:03d}: RMSE = {loss:.4f} | Params = {[theta_x, theta_v, theta_z, q_std, r_std]}\")\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = [theta_x, theta_v, theta_z, q_std, r_std]\n",
    "            print(\"✅ New best!\")\n",
    "\n",
    "    print(\"\\n🏆 Best RMSE on training:\", best_loss)\n",
    "    print(\"🎯 Best parameters:\", best_params)\n",
    "\n",
    "    return best_loss, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5873e27-0559-4e3e-869f-7e3316686c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss, best_params = tune_particle_filter_on_training(\n",
    "    X_train=X_train_50_week,\n",
    "    V_train=V_train_50_week,      # or None\n",
    "    ParticleClass=ParticleFilter,\n",
    "    n_trials=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a80df4-54c6-41e2-acaa-f2e9e23dfb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e866eb-02ee-4536-ba60-10d56c6ed488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pf = ParticleFilter(\n",
    "    num_particles=1000,\n",
    "    theta_x= 0.9025,\n",
    "    theta_v=0.09248,\n",
    "    theta_z= 0.9501,              # measurement gain\n",
    "    process_noise_std=5.49896,\n",
    "    measurement_noise_std= 42.3258\n",
    ")\n",
    "\n",
    "# 2) Fit on historical data, with or without V:\n",
    "filtered_df = pf.fit(X_train_50_week, V_matrix=V_train_50_week)\n",
    "\n",
    "# 3) Forecast next 24 hours:\n",
    "#    provide V_future for with‐V forecast, or omit for pure propagation\n",
    "forecast_df = pf.predict(V_future=V_test_50_week, steps=V_test_50_week.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74971401-d822-4c52-b497-9292e32e140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b950efa-40d3-4292-b301-8b67c5f6b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "households = [8, 11, 34]\n",
    "\n",
    "# --- 1) Compute RMSEs --- \n",
    "rmse_fit = []\n",
    "for i, h in enumerate(households):\n",
    "    true_train = X_train_50_week[:, i]\n",
    "    pred_train = filtered_df[f\"Filtered_{i}\"]\n",
    "    rmse_fit.append(mean_squared_error(true_train, pred_train, squared=False))\n",
    "\n",
    "rmse_fore = []\n",
    "for i, h in enumerate(households):\n",
    "    true_test = X_test_50_week[:, i]\n",
    "    pred_test = forecast_df[f\"Predicted_{i}\"]\n",
    "    rmse_fore.append(mean_squared_error(true_test, pred_test, squared=False))\n",
    "\n",
    "# --- 2) Plot Fitted vs True (training) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_train_50_week[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(filtered_df[f\"Filtered_{idx-1}\"],\n",
    "             label=f\"Fitted {h} (RMSE={rmse_fit[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fitted - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Plot Forecast vs True (test) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_test_50_week[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(forecast_df[f\"Predicted_{idx-1}\"],\n",
    "             label=f\"Forecast {h} (RMSE={rmse_fore[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b554ee-b8c9-428f-aa51-767b6445b4da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss, best_params = tune_particle_filter_on_training(\n",
    "    X_train=X_train_50,\n",
    "    V_train=V_train_50,      # or None\n",
    "    ParticleClass=ParticleFilter,\n",
    "    n_trials=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ae5d1-80d3-43ed-80c7-a735817a0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pf = ParticleFilter(\n",
    "    num_particles=1000,\n",
    "    theta_x=0.9025,\n",
    "    theta_v= 0.09248,\n",
    "    theta_z= 0.95012,              # measurement gain\n",
    "    process_noise_std=5.4989,\n",
    "    measurement_noise_std= 42.3258\n",
    "    )\n",
    "\n",
    "# 2) Fit on historical data, with or without V:\n",
    "filtered_df = pf.fit(X_train_50, V_matrix=V_train_50)\n",
    "\n",
    "# 3) Forecast next 24 hours:\n",
    "#    provide V_future for with‐V forecast, or omit for pure propagation\n",
    "forecast_df = pf.predict(V_future=V_test_50, steps=V_test_50.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4c278-9942-4f6a-ba42-de950963683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "households = [8, 11, 34]\n",
    "\n",
    "# --- 1) Compute RMSEs --- \n",
    "rmse_fit = []\n",
    "for i, h in enumerate(households):\n",
    "    true_train = X_train_50[:, i]\n",
    "    pred_train = filtered_df[f\"Filtered_{i}\"]\n",
    "    rmse_fit.append(mean_squared_error(true_train, pred_train, squared=False))\n",
    "\n",
    "rmse_fore = []\n",
    "for i, h in enumerate(households):\n",
    "    true_test = X_test_50[:, i]\n",
    "    pred_test = forecast_df[f\"Predicted_{i}\"]\n",
    "    rmse_fore.append(mean_squared_error(true_test, pred_test, squared=False))\n",
    "\n",
    "# --- 2) Plot Fitted vs True (training) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_train_50[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(filtered_df[f\"Filtered_{idx-1}\"],\n",
    "             label=f\"Fitted {h} (RMSE={rmse_fit[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fitted - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Plot Forecast vs True (test) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_test_50[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(forecast_df[f\"Predicted_{idx-1}\"],\n",
    "             label=f\"Forecast {h} (RMSE={rmse_fore[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8f960-7ae1-4752-9a00-52efcf4c5360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss, best_params = tune_particle_filter_on_training(\n",
    "    X_train=X_train_50_without_load,      # or None\n",
    "    ParticleClass=ParticleFilter,\n",
    "    n_trials=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9378a-624a-47e2-bd4b-0da11ff1eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pf = ParticleFilter(\n",
    "    num_particles=1000,\n",
    "    theta_x=0.9841,\n",
    "    theta_v=0,\n",
    "    theta_z=1.3609,              # measurement gain\n",
    "    process_noise_std=8.21,\n",
    "    measurement_noise_std=30.1722\n",
    ")\n",
    "\n",
    "# 2) Fit on historical data, with or without V:\n",
    "filtered_df = pf.fit(X_train_50_without_load)\n",
    "\n",
    "# 3) Forecast next 24 hours:\n",
    "#    provide V_future for with‐V forecast, or omit for pure propagation\n",
    "forecast_df = pf.predict(steps=X_test_50_without_load.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0d255-8bfd-40da-9f23-f4d68a147c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume you’ve already done:\n",
    "# filtered_df   = pf.fit(X_train_50_week, V_matrix=V_train_50_week)\n",
    "# forecast_df   = pf.predict(V_future=V_test_50_week, steps=V_test_50_week.shape[0])\n",
    "# and that the true series live in X_train_50_week (shape T×3) and X_test_50_week (shape H×3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "households = [8, 11, 34]\n",
    "\n",
    "# --- 1) Compute RMSEs --- \n",
    "rmse_fit = []\n",
    "for i, h in enumerate(households):\n",
    "    true_train = X_train_50_without_load[:, i]\n",
    "    pred_train = filtered_df[f\"Filtered_{i}\"]\n",
    "    rmse_fit.append(mean_squared_error(true_train, pred_train, squared=False))\n",
    "\n",
    "rmse_fore = []\n",
    "for i, h in enumerate(households):\n",
    "    true_test = X_test_50_without_load[:, i]\n",
    "    pred_test = forecast_df[f\"Predicted_{i}\"]\n",
    "    rmse_fore.append(mean_squared_error(true_test, pred_test, squared=False))\n",
    "\n",
    "# --- 2) Plot Fitted vs True (training) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_train_50_without_load[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(filtered_df[f\"Filtered_{idx-1}\"],\n",
    "             label=f\"Fitted {h} (RMSE={rmse_fit[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fitted - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Plot Forecast vs True (test) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_test_50_without_load[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(forecast_df[f\"Predicted_{idx-1}\"],\n",
    "             label=f\"Forecast {h} (RMSE={rmse_fore[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b8d4f-4891-49b9-af04-4be50a376777",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b14aa421-1b88-440f-88a5-ed8723ae6d03",
   "metadata": {},
   "source": [
    "## or V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98590d1e-a5c5-4849-8fb4-7bcefded6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_30 = {}\n",
    "\n",
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house = df_30[df_30[\"house_hold\"] == house_id].copy()\n",
    "    house[\"is_weekday\"] = house[\"is_weekday\"].astype(\"category\")\n",
    "\n",
    "\n",
    "    # Design matrix without intercept (remove intercept with -1)\n",
    "    X_interaction_2_separate = dmatrix(\n",
    "        \"C(is_weekday) * season \", data=house, return_type=\"dataframe\"\n",
    "    )\n",
    "\n",
    "\n",
    "    hour_cols_v2 = [f'v2_hour_{h}' for h in range (0,24)]\n",
    "    Y = house[hour_cols_v2].to_numpy()\n",
    "\n",
    "    try:\n",
    "        model_3 = _multivariate_ols_fit(Y, X_interaction_2_separate)\n",
    "        param_30[house_id] = model_3[0]\n",
    "\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipped household {house_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8d532-2e37-4689-9f2e-6c304b18e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_scores_hierarchical(param_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ff3bd-b9a4-4661-9436-f24403281298",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_with_week_weekend = cluster_and_pca(param_30)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "X_pca = clusters_with_week_weekend[\"pca_coordinates\"]\n",
    "labels = clusters_with_week_weekend[\"labels\"]\n",
    "\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Set1', s=50)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Households clustered with weekdays and weekends')\n",
    "ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "# Add silhouette score annotation in bottom right corner\n",
    "ax.text(\n",
    "    0.98, 0.02, \n",
    "    \"Silhouette Score = 0.62\", \n",
    "    transform=ax.transAxes,\n",
    "    ha='right', va='bottom',\n",
    "    fontsize=10, color='black'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf1849-85eb-4394-8f60-05b912622d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_week_0 = {}\n",
    "param_week_1 = {}\n",
    "weekday_predict_df={}\n",
    "weekend_predict_df={}\n",
    "\n",
    "for house_id in df_30[\"house_hold\"].unique():\n",
    "    house = df_30[df_30[\"house_hold\"] == house_id]\n",
    "\n",
    "    house_0 = house[house[\"is_weekday\"] == 0]\n",
    "    house_1 = house[house[\"is_weekday\"] == 1]\n",
    "\n",
    "    # Add intercept (constant term) to the season variable\n",
    "    X_main_effect_2_separate_0 = sm.add_constant(house_0[[\"season\"]].reset_index(drop=True))\n",
    "    X_main_effect_2_separate_1 = sm.add_constant(house_1[[\"season\"]].reset_index(drop=True))\n",
    "\n",
    "    hour_cols_v2 = [f'v2_hour_{h}' for h in range(24)]\n",
    "    Y_0 = house_0[hour_cols_v2].to_numpy()\n",
    "    Y_1 = house_1[hour_cols_v2].to_numpy()\n",
    "    \n",
    "    model_3_0 = _multivariate_ols_fit(Y_0, X_main_effect_2_separate_0)\n",
    "    model_3_1 = _multivariate_ols_fit(Y_1, X_main_effect_2_separate_1)\n",
    "    \n",
    "    param_week_0[house_id] = model_3_0[0]\n",
    "    param_week_1[house_id] = model_3_1[0]\n",
    "    weekday_predict_df[house_id] = X_main_effect_2_separate_0 @ model_3_0[0]\n",
    "    weekend_predict_df[house_id] = X_main_effect_2_separate_1 @ model_3_1[0]\n",
    "\n",
    "\n",
    "    pred_0 = pd.DataFrame(X_main_effect_2_separate_0 @ model_3_0[0])\n",
    "    pred_0.columns = hour_cols_v2\n",
    "\n",
    "    pred_0[\"house_hold\"] = house_id\n",
    "    pred_0[\"day\"] = house_0[\"day\"].values\n",
    "    weekday_predict_df[house_id] = pred_0\n",
    "\n",
    "    pred_1 = pd.DataFrame(X_main_effect_2_separate_1 @ model_3_1[0])\n",
    "    pred_1.columns = hour_cols_v2\n",
    "\n",
    "    pred_1[\"house_hold\"] = house_id\n",
    "    pred_1[\"day\"] = house_1[\"day\"].values\n",
    "    weekend_predict_df[house_id] = pred_1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667a532-d32a-4afc-a34b-cd9910185afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weekday= cluster_and_pca(param_week_0)\n",
    "weekend=cluster_and_pca(param_week_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8a17f-48ca-4882-a777-07977c0ce5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_silhouette_scores_hierarchical(param_week_0, title=\"Silhouette Scores for Weekdays\")\n",
    "plot_silhouette_scores_hierarchical(param_week_1, title=\"Silhouette Scores for Weekends\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d2778-56eb-431c-b476-651101e19616",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend[\"cluster_df\"][weekend[\"cluster_df\"][\"cluster\"] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d3c4a-9169-4425-8d6c-acdf68adc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday[\"cluster_df\"][weekday[\"cluster_df\"][\"cluster\"] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb2662-7901-47ac-9a23-49bb0e75cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Mapping semantic clusters to visual clusters\n",
    "# Assume these remappings based on your observation\n",
    "# Weekday: 0 = Red (Group A), 1 = Grey (Group B)\n",
    "# Weekend: 1 = Red (Group A), 0 = Grey (Group B)\n",
    "\n",
    "color_map = {0: 'grey', 1: 'red'}           # Weekday labels\n",
    "weekend_remap = {0: 1, 1: 0}                # Reverse mapping for weekend\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, data, label, sil_score, remap in zip(\n",
    "    axs,\n",
    "    [weekday, weekend],\n",
    "    [\"Weekday\", \"Weekend\"],\n",
    "    [0.63, 0.62],\n",
    "    [False, True]  # apply remap only to weekend\n",
    "):\n",
    "    X_pca = data[\"pca_coordinates\"]\n",
    "    clusters = data[\"labels\"]\n",
    "\n",
    "    # Remap if necessary (for weekend)\n",
    "    if remap:\n",
    "        clusters = [weekend_remap[int(c)] for c in clusters]\n",
    "\n",
    "    # Plot points with semantic cluster colors\n",
    "    for i in range(len(X_pca)):\n",
    "        color = color_map[int(clusters[i])]\n",
    "        ax.scatter(X_pca[i, 0], X_pca[i, 1], color=color, s=50)\n",
    "\n",
    "    ax.set_xlabel(\"PCA Component 1\")\n",
    "    ax.set_ylabel(\"PCA Component 2\")\n",
    "    ax.set_title(f\"Hierarchical Clustering: {label}\")\n",
    "\n",
    "    # Silhouette score\n",
    "    ax.text(\n",
    "        0.98, 0.02,\n",
    "        f\"Silhouette Score = {sil_score:.2f}\",\n",
    "        transform=ax.transAxes,\n",
    "        ha='right', va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "    # Semantic legend (consistent across both)\n",
    "    custom_legend = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Cluster 0',\n",
    "               markerfacecolor='red', markersize=8),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Cluster 1',\n",
    "               markerfacecolor='grey', markersize=8)\n",
    "    ]\n",
    "    ax.legend(handles=custom_legend, title=\"Cluster\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092e9e4-1561-4c68-87a4-62256cd41e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_30_rest= {}\n",
    "\n",
    "for house_id in df_3[\"house_hold\"].unique():\n",
    "    house = df_3[df_3[\"house_hold\"] == house_id].copy()\n",
    "    house[\"is_weekday\"] = house[\"is_weekday\"].astype(\"category\")\n",
    "\n",
    "\n",
    "    # Design matrix without intercept\n",
    "    X_interaction_2_separate = dmatrix(\n",
    "        \"C(is_weekday) * season \", data=house, return_type=\"dataframe\"\n",
    "    )\n",
    "\n",
    "\n",
    "    hour_cols_v2 = [f'v2_hour_{h}' for h in range (0,24)]\n",
    "    Y = house[hour_cols_v2].to_numpy()\n",
    "\n",
    "    try:\n",
    "        model_3 = _multivariate_ols_fit(Y, X_interaction_2_separate)\n",
    "        param_30_rest[house_id] = model_3[0]\n",
    "\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipped household {house_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110774a7-40f7-4244-8a42-daf0e9069c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_week_0_rest={}\n",
    "param_week_1_rest={}\n",
    "\n",
    "for house_id in df_3[\"house_hold\"].unique():\n",
    "    house = df_3[df_3[\"house_hold\"] == house_id]\n",
    "\n",
    "    house_0 = house[house[\"is_weekday\"] == 0]\n",
    "    house_1 = house[house[\"is_weekday\"] == 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X_main_effect_2_separate_0 = sm.add_constant(house_0[[\"season\"]])\n",
    "    X_main_effect_2_separate_1 = sm.add_constant(house_1[[\"season\"]])\n",
    "\n",
    "\n",
    "    hour_cols_v2 = [f'v2_hour_{h}' for h in range (0,24)]\n",
    "    Y_0 = house_0[hour_cols_v2].to_numpy()\n",
    "    Y_1 = house_1[hour_cols_v2].to_numpy()\n",
    "    \n",
    "    model_3_0 = _multivariate_ols_fit(Y_0, X_main_effect_2_separate_0)\n",
    "    model_3_1 = _multivariate_ols_fit(Y_1, X_main_effect_2_separate_1)\n",
    "    \n",
    "    param_week_0_rest[house_id]=model_3_0[0]\n",
    "    param_week_1_rest[house_id]=model_3_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcad3e0-54d1-4aa4-84e7-aa69c61689a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_clusters_to_new(param_30_rest, clusters_with_week_weekend[\"scaler\"], clusters_with_week_weekend[\"classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2a163-965b-4927-9782-2d2380d963bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_clusters_to_new(param_week_0_rest, weekday[\"scaler\"], weekday[\"classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f2c90-92b3-4864-9fc4-141940f3c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_clusters_to_new(param_week_1_rest, weekend[\"scaler\"], weekend[\"classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064e975-8186-4543-8b19-b2af5bb03ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_predict_df = pd.concat(weekend_predict_df.values(), ignore_index=True)\n",
    "weekday_predict_df = pd.concat(weekday_predict_df.values(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c55c5f-ea4d-42d5-bced-916a1382a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_to_cluster_weekday = weekday[\"cluster_df\"].set_index(\"house_id\")[\"cluster\"]\n",
    "house_to_cluster_weekend = weekend[\"cluster_df\"].set_index(\"house_id\")[\"cluster\"]\n",
    "\n",
    "# Step 2: Map clusters to prediction DataFrames\n",
    "weekday_predict_df[\"cluster\"] = weekday_predict_df[\"house_hold\"].map(house_to_cluster_weekday)\n",
    "weekend_predict_df[\"cluster\"] = weekend_predict_df[\"house_hold\"].map(house_to_cluster_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ac9a7-a1a0-4f31-be55-f1b303ca79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce44ab-be38-49cc-bdda-249eee4e4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_predict_df(predict_df):\n",
    "    # Step 1: Melt wide → long format\n",
    "    long_df = predict_df.melt(\n",
    "        id_vars=[\"house_hold\", \"day\", \"cluster\"],\n",
    "        value_vars=[f\"v2_hour_{h}\" for h in range(24)],\n",
    "        var_name=\"hour\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Extract hour number as integer\n",
    "    long_df[\"hour\"] = long_df[\"hour\"].str.extract(r\"v2_hour_(\\d+)\").astype(int)\n",
    "\n",
    "    # Step 3: Compute aggregated average value per cluster/day/hour\n",
    "    group_avg = (\n",
    "        long_df\n",
    "        .groupby([\"cluster\", \"day\", \"hour\"])[\"value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"value\": \"cluster_day_hour_avg\"})\n",
    "    )\n",
    "\n",
    "    # Step 4: Merge average back to long format\n",
    "    long_df_with_avg = long_df.merge(group_avg, on=[\"cluster\", \"day\", \"hour\"])\n",
    "\n",
    "    # Step 5: Group by house_hold, day, hour — keep raw value and cluster mean\n",
    "    household_hourly = (\n",
    "        long_df_with_avg\n",
    "        .groupby([\"house_hold\", \"day\", \"hour\"])\n",
    "        .agg(\n",
    "            raw_value=(\"value\", \"first\"),\n",
    "            cluster=(\"cluster\", \"first\"),\n",
    "            cluster_day_hour_avg=(\"cluster_day_hour_avg\", \"first\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return household_hourly\n",
    "\n",
    "# Apply function to both datasets\n",
    "weekday_hourly = process_predict_df(weekday_predict_df)\n",
    "weekend_hourly = process_predict_df(weekend_predict_df)\n",
    "\n",
    "# # Combine the results (optional)\n",
    "# combined_hourly = pd.concat([weekday_hourly, weekend_hourly], ignore_index=True)\n",
    "\n",
    "# # View sample\n",
    "# print(combined_hourly.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e46e7d-d6c9-476b-9e46-cdd3147b05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_hourly = process_predict_df(weekday_predict_df)\n",
    "weekend_hourly = process_predict_df(weekend_predict_df)\n",
    "\n",
    "\n",
    "\n",
    "weekday_profile = compute_cluster_profiles(weekday_hourly, 1)\n",
    "plot_cluster_profiles(weekday_profile, title_prefix=\"Weekday \", day=1)\n",
    "\n",
    "\n",
    "weekend_profile = compute_cluster_profiles(weekend_hourly, 5)\n",
    "plot_cluster_profiles(weekend_profile, title_prefix=\"Weekend \", day=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47274b2-d705-4340-b9fe-4b51a9ac7c11",
   "metadata": {},
   "source": [
    "## weekday kalman V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eda7c5-bbe2-4aa8-9d22-b01ed9739f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = weekday_hourly[weekday_hourly[\"cluster\"] == 1]\n",
    "\n",
    "load_profile_matrix = (\n",
    "    cluster_0.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "load_profile_flat = load_profile_matrix.values.flatten()\n",
    "\n",
    "available_days = load_profile_matrix.index.tolist()\n",
    "df_filtered = df_3[\n",
    "    df_3[\"day\"].isin(available_days) &\n",
    "    df_3[\"house_hold\"].isin([8, 11, 34])\n",
    "].sort_values(by=[\"house_hold\", \"day\"]).reset_index(drop=True)\n",
    "\n",
    "hour_cols = [f\"v2_hour_{i}\" for i in range(24)]\n",
    "households = [8, 11, 34]\n",
    "observed_matrix = np.vstack([\n",
    "    df_filtered[df_filtered[\"house_hold\"] == h][hour_cols].values.flatten()\n",
    "    for h in households\n",
    "]).T\n",
    "\n",
    "V_matrix = np.repeat(load_profile_flat.reshape(-1, 1), 3, axis=1)\n",
    "\n",
    "split_point_week = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50_week  = observed_matrix[:split_point]\n",
    "X_test_50_week = observed_matrix[split_point:]\n",
    "V_train_50_week  = V_matrix[:split_point]\n",
    "V_test_50_week = V_matrix[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d98c6-f75f-47e4-94ac-7baa1a4de9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "n_trials = 200\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Sample parameters uniformly from your desired ranges\n",
    "    Theta_X  = np.random.uniform(0.1, 0.99)\n",
    "    Theta_v  = np.random.uniform(0.01, 1.0)\n",
    "    theta_z  = np.random.uniform(0.1,  2.0)\n",
    "    q8  = np.random.uniform(0.01, 10.0)\n",
    "    q11 = np.random.uniform(0.01, 10.0)\n",
    "    q34 = np.random.uniform(0.01, 10.0)\n",
    "    r8  = np.random.uniform(0.1,  50.0)\n",
    "    r11 = np.random.uniform(0.1,  50.0)\n",
    "    r34 = np.random.uniform(0.1,  50.0)\n",
    "\n",
    "    params = [Theta_X, Theta_v, theta_z, q8, q11, q34, r8, r11, r34]\n",
    "    loss = multivariate_kalman_loss_per_household_qr(params, X_train_50_week, V_train_50_week, n_folds=3)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_params = params\n",
    "\n",
    "    print(f\"Trial {trial+1}/{n_trials}: RMSE = {loss:.2f}\")\n",
    "\n",
    "print(\"\\n🎯 Best parameters found:\")\n",
    "print(f\" Θₓ  = {best_params[0]:.4f}\")\n",
    "print(f\" Θᵥ  = {best_params[1]:.4f}\")\n",
    "print(f\" θ_z = {best_params[2]:.4f}\")\n",
    "print(f\" q₈  = {best_params[3]:.4f}, q₁₁ = {best_params[4]:.4f}, q₃₄ = {best_params[5]:.4f}\")\n",
    "print(f\" r₈  = {best_params[6]:.4f}, r₁₁ = {best_params[7]:.4f}, r₃₄ = {best_params[8]:.4f}\")\n",
    "print(f\"Best cross‐validated RMSE: {best_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e89b2a-23a5-4f16-8292-de68d6dd70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KalmanForecaster(\n",
    "    Theta_X=0.4145,\n",
    "    Theta_v=0.5878,\n",
    "    theta_z= 0.2477,\n",
    "    Q_diag=[9.7442,9.8622, 6.9846],\n",
    "    R_diag=[26.8512, 15.5454, 40.7084]\n",
    ")\n",
    "\n",
    "filtered_df_week = kf.fit(X_train_50_week, V_train_50_week)\n",
    "forecast_df_week = kf.forecast(V_test_50_week)\n",
    "\n",
    "# === Compute RMSE ===\n",
    "rmse_fit_8 = mean_squared_error(X_train_50_week[:, 0], filtered_df_week[\"Filtered_8\"], squared=False)\n",
    "rmse_fit_11 = mean_squared_error(X_train_50_week[:, 1], filtered_df_week[\"Filtered_11\"], squared=False)\n",
    "rmse_fit_34 = mean_squared_error(X_train_50_week[:, 2], filtered_df_week[\"Filtered_34\"], squared=False)\n",
    "\n",
    "rmse_fore_8 = mean_squared_error(X_test_50_week[:, 0], forecast_df_week[\"Forecast_8\"], squared=False)\n",
    "rmse_fore_11 = mean_squared_error(X_test_50_week[:, 1], forecast_df_week[\"Forecast_11\"], squared=False)\n",
    "rmse_fore_34 = mean_squared_error(X_test_50_week[:, 2], forecast_df_week[\"Forecast_34\"], squared=False)\n",
    "\n",
    "# === Plot: Fitted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_train_50_week[:, 0], label=\"True 8\")\n",
    "plt.plot(filtered_df_week[\"Filtered_8\"], label=f\"Fitted 8 (RMSE={rmse_fit_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_train_50_week[:, 1], label=\"True 11\")\n",
    "plt.plot(filtered_df_week[\"Filtered_11\"], label=f\"Fitted 11 (RMSE={rmse_fit_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_train_50_week[:, 2], label=\"True 34\")\n",
    "plt.plot(filtered_df_week[\"Filtered_34\"], label=f\"Fitted 34 (RMSE={rmse_fit_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot: Forecasted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_test_50_week[:, 0], label=\"True 8\")\n",
    "plt.plot(forecast_df_week[\"Forecast_8\"], label=f\"Forecast 8 (RMSE={rmse_fore_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_test_50_week[:, 1], label=\"True 11\")\n",
    "plt.plot(forecast_df_week[\"Forecast_11\"], label=f\"Forecast 11 (RMSE={rmse_fore_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_test_50_week[:, 2], label=\"True 34\")\n",
    "plt.plot(forecast_df_week[\"Forecast_34\"], label=f\"Forecast 34 (RMSE={rmse_fore_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abae94-34fb-4889-8f06-43dee4c65155",
   "metadata": {},
   "source": [
    "## weekend kalman v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b363ee-85c9-409c-9f7a-26ab17788e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cluster 0 and cluster 1 profiles\n",
    "cluster_0 = weekend_hourly[weekend_hourly[\"cluster\"] == 0]\n",
    "cluster_1 = weekend_hourly[weekend_hourly[\"cluster\"] == 1]\n",
    "\n",
    "# Construct load profile matrices\n",
    "profile_0 = (\n",
    "    cluster_0.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "profile_1 = (\n",
    "    cluster_1.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Flatten each profile\n",
    "flat_0 = profile_0.values.flatten()\n",
    "flat_1 = profile_1.values.flatten()\n",
    "\n",
    "# Combine into V matrix (household 8 & 34 use cluster 0, household 11 uses cluster 1)\n",
    "V_matrix = np.stack([\n",
    "    flat_0,       # Household 8 (cluster 0)\n",
    "    flat_1,       # Household 11 (cluster 1)\n",
    "    flat_0        # Household 34 (cluster 0)\n",
    "], axis=1)\n",
    "\n",
    "# Filter observed data to match available days\n",
    "available_days = profile_0.index.tolist()  # assumes profile_0 and profile_1 use the same days\n",
    "df_filtered = df_3[\n",
    "    df_3[\"day\"].isin(available_days) &\n",
    "    df_3[\"house_hold\"].isin([8, 11, 34])\n",
    "].sort_values(by=[\"house_hold\", \"day\"]).reset_index(drop=True)\n",
    "\n",
    "# Create observed matrix\n",
    "hour_cols = [f\"v2_hour_{i}\" for i in range(24)]\n",
    "households = [8, 11, 34]\n",
    "observed_matrix = np.vstack([\n",
    "    df_filtered[df_filtered[\"house_hold\"] == h][hour_cols].values.flatten()\n",
    "    for h in households\n",
    "]).T\n",
    "\n",
    "# Split into training and test sets\n",
    "split_point = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50 = observed_matrix[:split_point]\n",
    "X_test_50 = observed_matrix[split_point:]\n",
    "V_train_50 = V_matrix[:split_point]\n",
    "V_test_50 = V_matrix[split_point:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0f973-6233-46ab-b51b-cd2f0d899f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Number of random trials\n",
    "n_trials = 200\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Sample parameters uniformly from your desired ranges\n",
    "    Theta_X  = np.random.uniform(0.1, 0.99)\n",
    "    Theta_v  = np.random.uniform(0.01, 1.0)\n",
    "    theta_z  = np.random.uniform(0.1,  2.0)\n",
    "    q8  = np.random.uniform(0.01, 10.0)\n",
    "    q11 = np.random.uniform(0.01, 10.0)\n",
    "    q34 = np.random.uniform(0.01, 10.0)\n",
    "    r8  = np.random.uniform(0.1,  50.0)\n",
    "    r11 = np.random.uniform(0.1,  50.0)\n",
    "    r34 = np.random.uniform(0.1,  50.0)\n",
    "\n",
    "    params = [Theta_X, Theta_v, theta_z, q8, q11, q34, r8, r11, r34]\n",
    "    loss = multivariate_kalman_loss_per_household_qr(params, X_train_50, V_train_50, n_folds=3)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_params = params\n",
    "\n",
    "    print(f\"Trial {trial+1}/{n_trials}: RMSE = {loss:.2f}\")\n",
    "\n",
    "print(\"\\n🎯 Best parameters found:\")\n",
    "print(f\" Θₓ  = {best_params[0]:.4f}\")\n",
    "print(f\" Θᵥ  = {best_params[1]:.4f}\")\n",
    "print(f\" θ_z = {best_params[2]:.4f}\")\n",
    "print(f\" q₈  = {best_params[3]:.4f}, q₁₁ = {best_params[4]:.4f}, q₃₄ = {best_params[5]:.4f}\")\n",
    "print(f\" r₈  = {best_params[6]:.4f}, r₁₁ = {best_params[7]:.4f}, r₃₄ = {best_params[8]:.4f}\")\n",
    "print(f\"Best cross‐validated RMSE: {best_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56292a8e-9798-4a41-8511-ae217a5e3772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# === Fit Kalman Filter on Training ===\n",
    "kf = KalmanForecaster(\n",
    "    Theta_X= 0.3222,\n",
    "    Theta_v=0.4945,\n",
    "    theta_z= 0.5203,\n",
    "    Q_diag=[7.0276,9.4412, 0.4039],\n",
    "    R_diag=[35.3082, 46.2699,9.1107]\n",
    ")\n",
    "\n",
    "filtered_df = kf.fit(X_train_50, V_train_50)\n",
    "forecast_df = kf.forecast(V_test_50)\n",
    "\n",
    "# === Compute RMSE ===\n",
    "rmse_fit_8 = mean_squared_error(X_train_50[:, 0], filtered_df[\"Filtered_8\"], squared=False)\n",
    "rmse_fit_11 = mean_squared_error(X_train_50[:, 1], filtered_df[\"Filtered_11\"], squared=False)\n",
    "rmse_fit_34 = mean_squared_error(X_train_50[:, 2], filtered_df[\"Filtered_34\"], squared=False)\n",
    "\n",
    "rmse_fore_8 = mean_squared_error(X_test_50[:, 0], forecast_df[\"Forecast_8\"], squared=False)\n",
    "rmse_fore_11 = mean_squared_error(X_test_50[:, 1], forecast_df[\"Forecast_11\"], squared=False)\n",
    "rmse_fore_34 = mean_squared_error(X_test_50[:, 2], forecast_df[\"Forecast_34\"], squared=False)\n",
    "\n",
    "# === Plot: Fitted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_train_50[:, 0], label=\"True 8\")\n",
    "plt.plot(filtered_df[\"Filtered_8\"], label=f\"Fitted 8 (RMSE={rmse_fit_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_train_50[:, 1], label=\"True 11\")\n",
    "plt.plot(filtered_df[\"Filtered_11\"], label=f\"Fitted 11 (RMSE={rmse_fit_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_train_50[:, 2], label=\"True 34\")\n",
    "plt.plot(filtered_df[\"Filtered_34\"], label=f\"Fitted 34 (RMSE={rmse_fit_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot: Forecasted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_test_50[:, 0], label=\"True 8\")\n",
    "plt.plot(forecast_df[\"Forecast_8\"], label=f\"Forecast 8 (RMSE={rmse_fore_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_test_50[:, 1], label=\"True 11\")\n",
    "plt.plot(forecast_df[\"Forecast_11\"], label=f\"Forecast 11 (RMSE={rmse_fore_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_test_50[:, 2], label=\"True 34\")\n",
    "plt.plot(forecast_df[\"Forecast_34\"], label=f\"Forecast 34 (RMSE={rmse_fore_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259203b-5201-470e-924c-aac044bce2f4",
   "metadata": {},
   "source": [
    "## v2 kalman without load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6071a-73bc-41c5-919f-90491e82090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the hourly columns and target households\n",
    "hour_columns = [f\"v2_hour_{i}\" for i in range(24)]\n",
    "households   = [8, 11, 34]\n",
    "\n",
    "# 2) For each household, sort by day and flatten its 365×24 values into one long series\n",
    "series_list = []\n",
    "for hh in households:\n",
    "    hh_df = df_3[df_3[\"house_hold\"] == hh].sort_values(\"day\")\n",
    "    # ensure 365 days × 24 hours\n",
    "    assert hh_df.shape[0] == 365, f\"Expected 365 days for HH {hh}, got {hh_df.shape[0]}\"\n",
    "    flat = hh_df[hour_columns].to_numpy().reshape(-1)  # length = 365*24\n",
    "    series_list.append(flat)\n",
    "\n",
    "# 3) Stack into an observed‐data matrix of shape (8760, 3)\n",
    "observed_matrix = np.vstack(series_list).T\n",
    "\n",
    "split_point_without_load = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50_without_load  = observed_matrix[:split_point]\n",
    "X_test_50_without_load = observed_matrix[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17773e06-4e94-4b38-982f-cfe8b5d84de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Number of random trials\n",
    "n_trials = 200\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Sample parameters uniformly from your desired ranges\n",
    "    Theta_X  = np.random.uniform(0.1, 0.99)\n",
    "    Theta_v  = np.random.uniform(0.01, 1.0)\n",
    "    theta_z  = np.random.uniform(0.1,  2.0)\n",
    "    q8  = np.random.uniform(0.01, 10.0)\n",
    "    q11 = np.random.uniform(0.01, 10.0)\n",
    "    q34 = np.random.uniform(0.01, 10.0)\n",
    "    r8  = np.random.uniform(0.1,  50.0)\n",
    "    r11 = np.random.uniform(0.1,  50.0)\n",
    "    r34 = np.random.uniform(0.1,  50.0)\n",
    "\n",
    "    params = [Theta_X, Theta_v, theta_z, q8, q11, q34, r8, r11, r34]\n",
    "    loss = multivariate_kalman_loss_per_household_qr(params, X_train_50_without_load, n_folds=3)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_params = params\n",
    "\n",
    "    print(f\"Trial {trial+1}/{n_trials}: RMSE = {loss:.2f}\")\n",
    "\n",
    "print(\"\\n🎯 Best parameters found:\")\n",
    "print(f\" Θₓ  = {best_params[0]:.4f}\")\n",
    "print(f\" Θᵥ  = {best_params[1]:.4f}\")\n",
    "print(f\" θ_z = {best_params[2]:.4f}\")\n",
    "print(f\" q₈  = {best_params[3]:.4f}, q₁₁ = {best_params[4]:.4f}, q₃₄ = {best_params[5]:.4f}\")\n",
    "print(f\" r₈  = {best_params[6]:.4f}, r₁₁ = {best_params[7]:.4f}, r₃₄ = {best_params[8]:.4f}\")\n",
    "print(f\"Best cross‐validated RMSE: {best_loss:.2f}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc33f8-5dd1-485f-8e5f-57d76e6c7800",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KalmanForecaster(\n",
    "    Theta_X= 0.9890,\n",
    "    Theta_v=0.3334,\n",
    "    theta_z= 1.5215,\n",
    "    Q_diag=[8.0686,8.5817, 9.9763],\n",
    "    R_diag=[12.1507,2.1135, 20.6185]\n",
    ")\n",
    "\n",
    "filtered_df_without_load = kf.fit(X_train_50_without_load)\n",
    "forecast_df_without_load = kf.forecast(steps=X_test_50_without_load.shape[0])\n",
    "\n",
    "# === Compute RMSE ===\n",
    "rmse_fit_8 = mean_squared_error(X_train_50_without_load[:, 0], filtered_df_without_load[\"Filtered_8\"], squared=False)\n",
    "rmse_fit_11 = mean_squared_error(X_train_50_without_load[:, 1], filtered_df_without_load[\"Filtered_11\"], squared=False)\n",
    "rmse_fit_34 = mean_squared_error(X_train_50_without_load[:, 2], filtered_df_without_load[\"Filtered_34\"], squared=False)\n",
    "\n",
    "rmse_fore_8 = mean_squared_error(X_test_50_without_load[:, 0], forecast_df_without_load[\"Forecast_8\"], squared=False)\n",
    "rmse_fore_11 = mean_squared_error(X_test_50_without_load[:, 1], forecast_df_without_load[\"Forecast_11\"], squared=False)\n",
    "rmse_fore_34 = mean_squared_error(X_test_50_without_load[:, 2], forecast_df_without_load[\"Forecast_34\"], squared=False)\n",
    "\n",
    "# === Plot: Fitted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_train_50_without_load[:, 0], label=\"True 8\")\n",
    "plt.plot(filtered_df_without_load[\"Filtered_8\"], label=f\"Fitted 8 (RMSE={rmse_fit_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_train_50_without_load[:, 1], label=\"True 11\")\n",
    "plt.plot(filtered_df_without_load[\"Filtered_11\"], label=f\"Fitted 11 (RMSE={rmse_fit_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_train_50_without_load[:, 2], label=\"True 34\")\n",
    "plt.plot(filtered_df_without_load[\"Filtered_34\"], label=f\"Fitted 34 (RMSE={rmse_fit_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot: Forecasted Values ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(X_test_50_without_load[:, 0], label=\"True 8\")\n",
    "plt.plot(forecast_df_without_load[\"Forecast_8\"], label=f\"Forecast 8 (RMSE={rmse_fore_8:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 8\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(X_test_50_without_load[:, 1], label=\"True 11\")\n",
    "plt.plot(forecast_df_without_load[\"Forecast_11\"], label=f\"Forecast 11 (RMSE={rmse_fore_11:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 11\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(X_test_50_without_load[:, 2], label=\"True 34\")\n",
    "plt.plot(forecast_df_without_load[\"Forecast_34\"], label=f\"Forecast 34 (RMSE={rmse_fore_34:.2f})\")\n",
    "plt.legend()\n",
    "plt.title(\"Forecast - Household 34\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d9210-509a-4b70-8c2d-c609435ab4dc",
   "metadata": {},
   "source": [
    "## particle filter v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78786e-6912-46ac-8627-0d5275a06da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = weekday_hourly[weekday_hourly[\"cluster\"] == 1]\n",
    "\n",
    "load_profile_matrix = (\n",
    "    cluster_0.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "load_profile_flat = load_profile_matrix.values.flatten()\n",
    "\n",
    "available_days = load_profile_matrix.index.tolist()\n",
    "df_filtered = df_3[\n",
    "    df_3[\"day\"].isin(available_days) &\n",
    "    df_3[\"house_hold\"].isin([8, 11, 34])\n",
    "].sort_values(by=[\"house_hold\", \"day\"]).reset_index(drop=True)\n",
    "\n",
    "hour_cols = [f\"v2_hour_{i}\" for i in range(24)]\n",
    "households = [8, 11, 34]\n",
    "observed_matrix = np.vstack([\n",
    "    df_filtered[df_filtered[\"house_hold\"] == h][hour_cols].values.flatten()\n",
    "    for h in households\n",
    "]).T\n",
    "\n",
    "V_matrix = np.repeat(load_profile_flat.reshape(-1, 1), 3, axis=1)\n",
    "\n",
    "split_point_week = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50_week  = observed_matrix[:split_point]\n",
    "X_test_50_week = observed_matrix[split_point:]\n",
    "V_train_50_week  = V_matrix[:split_point]\n",
    "V_test_50_week = V_matrix[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569e17a-dae8-4d39-a5c0-64fdad03d1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb16a0-f210-4b67-856e-3b14c26b6682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916fa8b-43fb-4822-b62b-f509f58d9f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a6cf8-949e-4ffc-84d4-166b09c31eed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss, best_params = tune_particle_filter_on_training(\n",
    "    X_train=X_train_50_week,\n",
    "    V_train=V_train_50_week,      # or None\n",
    "    ParticleClass=ParticleFilter,\n",
    "    n_trials=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea403b74-ef1f-4666-8a51-885caca9eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = ParticleFilter(\n",
    "    num_particles=1000,\n",
    "    theta_x=0.8,\n",
    "    theta_v=0.2,\n",
    "    theta_z=1.81,              # measurement gain\n",
    "    process_noise_std=6.88,\n",
    "    measurement_noise_std= 37.882\n",
    ")\n",
    "\n",
    "# 2) Fit on historical data, with or without V:\n",
    "filtered_df = pf.fit(X_train_50_week, V_matrix=V_train_50_week)\n",
    "\n",
    "# 3) Forecast next 24 hours:\n",
    "#    provide V_future for with‐V forecast, or omit for pure propagation\n",
    "forecast_df = pf.predict(V_future=V_test_50_week, steps=V_test_50_week.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816452c0-eed0-4d7b-9707-fe8b8d5c5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "households = [8, 11, 34]\n",
    "\n",
    "# --- 1) Compute RMSEs --- \n",
    "rmse_fit = []\n",
    "for i, h in enumerate(households):\n",
    "    true_train = X_train_50_week[:, i]\n",
    "    pred_train = filtered_df[f\"Filtered_{i}\"]\n",
    "    rmse_fit.append(mean_squared_error(true_train, pred_train, squared=False))\n",
    "\n",
    "rmse_fore = []\n",
    "for i, h in enumerate(households):\n",
    "    true_test = X_test_50_week[:, i]\n",
    "    pred_test = forecast_df[f\"Predicted_{i}\"]\n",
    "    rmse_fore.append(mean_squared_error(true_test, pred_test, squared=False))\n",
    "\n",
    "# --- 2) Plot Fitted vs True (training) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_train_50_week[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(filtered_df[f\"Filtered_{idx-1}\"],\n",
    "             label=f\"Fitted {h} (RMSE={rmse_fit[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fitted - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Plot Forecast vs True (test) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_test_50_week[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(forecast_df[f\"Predicted_{idx-1}\"],\n",
    "             label=f\"Forecast {h} (RMSE={rmse_fore[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04f7ec-8d8a-4122-9990-9b9fcfe38ce5",
   "metadata": {},
   "source": [
    "## particle filter v2 weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d18983-0c59-4e94-8dc9-adcc62c1ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = weekend_hourly[weekend_hourly[\"cluster\"] == 0]\n",
    "cluster_1 = weekend_hourly[weekend_hourly[\"cluster\"] == 1]\n",
    "\n",
    "# Construct load profile matrices\n",
    "profile_0 = (\n",
    "    cluster_0.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "profile_1 = (\n",
    "    cluster_1.groupby([\"day\", \"hour\"])[\"cluster_day_hour_avg\"]\n",
    "    .first()\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Flatten each profile\n",
    "flat_0 = profile_0.values.flatten()\n",
    "flat_1 = profile_1.values.flatten()\n",
    "\n",
    "# Combine into V matrix (household 8 & 34 use cluster 0, household 11 uses cluster 1)\n",
    "V_matrix = np.stack([\n",
    "    flat_0,       # Household 8 (cluster 0)\n",
    "    flat_1,       # Household 11 (cluster 1)\n",
    "    flat_0        # Household 34 (cluster 0)\n",
    "], axis=1)\n",
    "\n",
    "# Filter observed data to match available days\n",
    "available_days = profile_0.index.tolist()  # assumes profile_0 and profile_1 use the same days\n",
    "df_filtered = df_3[\n",
    "    df_3[\"day\"].isin(available_days) &\n",
    "    df_3[\"house_hold\"].isin([8, 11, 34])\n",
    "].sort_values(by=[\"house_hold\", \"day\"]).reset_index(drop=True)\n",
    "\n",
    "# Create observed matrix\n",
    "hour_cols = [f\"v2_hour_{i}\" for i in range(24)]\n",
    "households = [8, 11, 34]\n",
    "observed_matrix = np.vstack([\n",
    "    df_filtered[df_filtered[\"house_hold\"] == h][hour_cols].values.flatten()\n",
    "    for h in households\n",
    "]).T\n",
    "\n",
    "# Split into training and test sets\n",
    "split_point = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50 = observed_matrix[:split_point]\n",
    "X_test_50 = observed_matrix[split_point:]\n",
    "V_train_50 = V_matrix[:split_point]\n",
    "V_test_50 = V_matrix[split_point:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bdc1f-d52c-4e06-b2b5-c1dda3af15d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss, best_params = tune_particle_filter_on_training(\n",
    "    X_train=X_train_50,\n",
    "    V_train=V_train_50,      # or None\n",
    "    ParticleClass=ParticleFilter,\n",
    "    n_trials=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07029b-a062-4016-a648-cfe93abd3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = ParticleFilter(\n",
    "    num_particles=1000,\n",
    "    theta_x=0.9025,\n",
    "    theta_v= 0.09248,\n",
    "    theta_z=0.9501,              # measurement gain\n",
    "    process_noise_std=5.49896,\n",
    "    measurement_noise_std= 42.325\n",
    ")\n",
    "\n",
    "# 2) Fit on historical data, with or without V:\n",
    "filtered_df = pf.fit(X_train_50, V_matrix=V_train_50)\n",
    "\n",
    "# 3) Forecast next 24 hours:\n",
    "#    provide V_future for with‐V forecast, or omit for pure propagation\n",
    "forecast_df = pf.predict(V_future=V_test_50, steps=V_test_50.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f6f14-e4f8-4971-91ac-ba0912f4273f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "households = [8, 11, 34]\n",
    "\n",
    "# --- 1) Compute RMSEs --- \n",
    "rmse_fit = []\n",
    "for i, h in enumerate(households):\n",
    "    true_train = X_train_50[:, i]\n",
    "    pred_train = filtered_df[f\"Filtered_{i}\"]\n",
    "    rmse_fit.append(mean_squared_error(true_train, pred_train, squared=False))\n",
    "\n",
    "rmse_fore = []\n",
    "for i, h in enumerate(households):\n",
    "    true_test = X_test_50[:, i]\n",
    "    pred_test = forecast_df[f\"Predicted_{i}\"]\n",
    "    rmse_fore.append(mean_squared_error(true_test, pred_test, squared=False))\n",
    "\n",
    "# --- 2) Plot Fitted vs True (training) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_train_50[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(filtered_df[f\"Filtered_{idx-1}\"],\n",
    "             label=f\"Fitted {h} (RMSE={rmse_fit[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fitted - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Plot Forecast vs True (test) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_test_50[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(forecast_df[f\"Predicted_{idx-1}\"],\n",
    "             label=f\"Forecast {h} (RMSE={rmse_fore[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b098b-c7c0-499f-807b-f632f1e52433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the hourly columns and target households\n",
    "hour_columns = [f\"v2_hour_{i}\" for i in range(24)]\n",
    "households   = [8, 11, 34]\n",
    "\n",
    "# 2) For each household, sort by day and flatten its 365×24 values into one long series\n",
    "series_list = []\n",
    "for hh in households:\n",
    "    hh_df = df_3[df_3[\"house_hold\"] == hh].sort_values(\"day\")\n",
    "    # ensure 365 days × 24 hours\n",
    "    assert hh_df.shape[0] == 365, f\"Expected 365 days for HH {hh}, got {hh_df.shape[0]}\"\n",
    "    flat = hh_df[hour_columns].to_numpy().reshape(-1)  # length = 365*24\n",
    "    series_list.append(flat)\n",
    "\n",
    "# 3) Stack into an observed‐data matrix of shape (8760, 3)\n",
    "observed_matrix = np.vstack(series_list).T\n",
    "\n",
    "split_point_without_load = int(0.5 * observed_matrix.shape[0])\n",
    "X_train_50_without_load  = observed_matrix[:split_point]\n",
    "X_test_50_without_load = observed_matrix[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a516e-8ff9-4868-9461-c9a1e5bca641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss, best_params = tune_particle_filter_on_training(\n",
    "    X_train=X_train_50_without_load,   \n",
    "    ParticleClass=ParticleFilter,\n",
    "    n_trials=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d81d10-5b96-4aba-8706-60231d4be032",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = ParticleFilter(\n",
    "    num_particles=1000,\n",
    "    theta_x=0.98,\n",
    "    theta_v=0.2,\n",
    "    theta_z=1.360,              # measurement gain\n",
    "    process_noise_std=8.21,\n",
    "    measurement_noise_std=30.17\n",
    ")\n",
    "\n",
    "# 2) Fit on historical data, with or without V:\n",
    "filtered_df = pf.fit(X_train_50_without_load)\n",
    "\n",
    "# 3) Forecast next 24 hours:\n",
    "#    provide V_future for with‐V forecast, or omit for pure propagation\n",
    "forecast_df = pf.predict(steps=X_test_50_without_load.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1af34-479f-49b2-a565-048bada585d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "households = [8, 11, 34]\n",
    "\n",
    "# --- 1) Compute RMSEs --- \n",
    "rmse_fit = []\n",
    "for i, h in enumerate(households):\n",
    "    true_train = X_train_50_without_load[:, i]\n",
    "    pred_train = filtered_df[f\"Filtered_{i}\"]\n",
    "    rmse_fit.append(mean_squared_error(true_train, pred_train, squared=False))\n",
    "\n",
    "rmse_fore = []\n",
    "for i, h in enumerate(households):\n",
    "    true_test = X_test_50_without_load[:, i]\n",
    "    pred_test = forecast_df[f\"Predicted_{i}\"]\n",
    "    rmse_fore.append(mean_squared_error(true_test, pred_test, squared=False))\n",
    "\n",
    "# --- 2) Plot Fitted vs True (training) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_train_50_without_load[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(filtered_df[f\"Filtered_{idx-1}\"],\n",
    "             label=f\"Fitted {h} (RMSE={rmse_fit[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fitted - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Plot Forecast vs True (test) ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "for idx, h in enumerate(households, start=1):\n",
    "    plt.subplot(3, 1, idx)\n",
    "    plt.plot(X_test_50_without_load[:, idx-1], label=f\"True {h}\")\n",
    "    plt.plot(forecast_df[f\"Predicted_{idx-1}\"],\n",
    "             label=f\"Forecast {h} (RMSE={rmse_fore[idx-1]:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast - Household {h}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e896bec-4bee-417f-8e11-b6ae541619b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b0a94-f742-49d1-acc7-212869026db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75a3ee-53ed-4558-830e-0fa67c61171c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38071baa-9dd5-47c1-b187-884d6eb5a8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643151e-5f59-466f-a66f-fc2aa64eef18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf8606-b28a-426a-bda8-e30219e29da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
